{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3935,"sourceType":"datasetVersion","datasetId":2334}],"dockerImageVersionId":29271,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)","metadata":{}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:10.987317Z","iopub.execute_input":"2023-11-19T07:50:10.988013Z","iopub.status.idle":"2023-11-19T07:50:12.373820Z","shell.execute_reply.started":"2023-11-19T07:50:10.987920Z","shell.execute_reply":"2023-11-19T07:50:12.372309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_score, recall_score\n\n# Load datasets\ncheckins = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1')\ntips = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1')\ntags = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1')\n\n# Merge datasets\nmerged_data = pd.merge(checkins, tips, on=['user_ID', 'venue_ID'], how='outer')\nmerged_data = pd.merge(merged_data, tags, on='venue_ID', how='outer')\n\n# Create feature vectors\nfeatures = merged_data.groupby('venue_ID').size().reset_index(name='checkin_count')\n\n# Split dataset into training and testing sets\ntrain, test = train_test_split(features, test_size=0.2, random_state=42)\n\n# Fit k-NN model\nknn = NearestNeighbors(n_neighbors=5)  # You can adjust the number of neighbors\nknn.fit(train[['checkin_count']])\n\n# Find nearest neighbors for test instances\ndistances, indices = knn.kneighbors(test[['checkin_count']])\n\n# Evaluate precision and recall using ROC curve\n# Assuming a binary classification problem (e.g., recommending or not recommending a venue)\n# You need to define your own threshold for recommendation based on the problem requirements\n\n# Example threshold, adjust according to your problem\nthreshold = 30\n\n\n# Convert distances to binary predictions\npredictions = (distances < threshold).astype(int)\n\n# Calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(test['checkin_count'] >= threshold, predictions.sum(axis=1) > 0)\n\n# Choose a higher precision threshold\ntarget_precision = 3  # Adjust as needed\n\n# Find the index of the closest precision to the target\nbest_threshold_index = next((i for i, p in enumerate(precision) if p >= target_precision), len(precision) - 1)\n\n# Use the threshold that achieves the desired precision or the maximum available threshold\nbest_threshold = thresholds[best_threshold_index] if best_threshold_index < len(thresholds) else thresholds[-1]\n\n# Recalculate precision and recall with the best threshold\nupdated_predictions = (distances < best_threshold).astype(int)\nupdated_precision = precision_score(test['checkin_count'] >= threshold, updated_predictions.sum(axis=1) > 0)\nupdated_recall = recall_score(test['checkin_count'] >= threshold, updated_predictions.sum(axis=1) > 0)\n\nprint(f'Updated Precision: {updated_precision:.4f}')\nprint(f'Updated Recall: {updated_recall:.4f}')\nprint(f'Used Threshold: {best_threshold:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T09:34:44.977182Z","iopub.execute_input":"2023-11-19T09:34:44.977913Z","iopub.status.idle":"2023-11-19T09:34:45.116386Z","shell.execute_reply.started":"2023-11-19T09:34:44.977836Z","shell.execute_reply":"2023-11-19T09:34:45.115184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 3 csv files in the current version of the dataset:\n","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:12.376414Z","iopub.execute_input":"2023-11-19T07:50:12.376796Z","iopub.status.idle":"2023-11-19T07:50:12.396866Z","shell.execute_reply.started":"2023-11-19T07:50:12.376729Z","shell.execute_reply":"2023-11-19T07:50:12.395830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","metadata":{}},{"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-19T07:50:13.368448Z","iopub.execute_input":"2023-11-19T07:50:13.368954Z","iopub.status.idle":"2023-11-19T07:50:13.386473Z","shell.execute_reply.started":"2023-11-19T07:50:13.368852Z","shell.execute_reply":"2023-11-19T07:50:13.384999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-19T07:50:14.027460Z","iopub.execute_input":"2023-11-19T07:50:14.027912Z","iopub.status.idle":"2023-11-19T07:50:14.043785Z","shell.execute_reply.started":"2023-11-19T07:50:14.027852Z","shell.execute_reply":"2023-11-19T07:50:14.042587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-19T07:50:14.611196Z","iopub.execute_input":"2023-11-19T07:50:14.611623Z","iopub.status.idle":"2023-11-19T07:50:14.625722Z","shell.execute_reply.started":"2023-11-19T07:50:14.611543Z","shell.execute_reply":"2023-11-19T07:50:14.624687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you're ready to read in the data and use the plotting functions to visualize the data.","metadata":{}},{"cell_type":"markdown","source":"### Let's check 1st file: /kaggle/input/NY_Restauraunts_checkins.csv","metadata":{}},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# NY_Restauraunts_checkins.csv has 27149 rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'NY_Restauraunts_checkins.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:17.033662Z","iopub.execute_input":"2023-11-19T07:50:17.034200Z","iopub.status.idle":"2023-11-19T07:50:17.064810Z","shell.execute_reply.started":"2023-11-19T07:50:17.034115Z","shell.execute_reply":"2023-11-19T07:50:17.063636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:","metadata":{}},{"cell_type":"code","source":"df1.head(5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:18.439531Z","iopub.execute_input":"2023-11-19T07:50:18.439974Z","iopub.status.idle":"2023-11-19T07:50:18.466001Z","shell.execute_reply.started":"2023-11-19T07:50:18.439908Z","shell.execute_reply":"2023-11-19T07:50:18.464497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:","metadata":{}},{"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:19.274495Z","iopub.execute_input":"2023-11-19T07:50:19.275004Z","iopub.status.idle":"2023-11-19T07:50:19.302276Z","shell.execute_reply.started":"2023-11-19T07:50:19.274911Z","shell.execute_reply":"2023-11-19T07:50:19.301269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation matrix:","metadata":{}},{"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:19.749757Z","iopub.execute_input":"2023-11-19T07:50:19.750249Z","iopub.status.idle":"2023-11-19T07:50:20.289922Z","shell.execute_reply.started":"2023-11-19T07:50:19.750186Z","shell.execute_reply":"2023-11-19T07:50:20.288262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatter and density plots:","metadata":{}},{"cell_type":"code","source":"plotScatterMatrix(df1, 6, 15)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:20.293073Z","iopub.execute_input":"2023-11-19T07:50:20.293997Z","iopub.status.idle":"2023-11-19T07:50:21.323057Z","shell.execute_reply.started":"2023-11-19T07:50:20.293883Z","shell.execute_reply":"2023-11-19T07:50:21.321404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's check 2nd file: /kaggle/input/NY_Restauraunts_tags.csv","metadata":{}},{"cell_type":"code","source":"nRowsRead = 1000\n\n# NY_Restauraunts_tags.csv has 3298 rows in reality, but we are only loading/previewing the first 1000 rows\ndf2 = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ndf2.dataframeName = 'NY_Restauraunts_tags.csv'\n\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:21.326395Z","iopub.execute_input":"2023-11-19T07:50:21.327294Z","iopub.status.idle":"2023-11-19T07:50:21.367747Z","shell.execute_reply.started":"2023-11-19T07:50:21.327203Z","shell.execute_reply":"2023-11-19T07:50:21.366289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:","metadata":{}},{"cell_type":"code","source":"df2.head(5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:21.714430Z","iopub.execute_input":"2023-11-19T07:50:21.715045Z","iopub.status.idle":"2023-11-19T07:50:21.729340Z","shell.execute_reply.started":"2023-11-19T07:50:21.714980Z","shell.execute_reply":"2023-11-19T07:50:21.727810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:","metadata":{}},{"cell_type":"code","source":"plotPerColumnDistribution(df2, 10, 5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:22.177525Z","iopub.execute_input":"2023-11-19T07:50:22.178321Z","iopub.status.idle":"2023-11-19T07:50:22.194432Z","shell.execute_reply.started":"2023-11-19T07:50:22.178258Z","shell.execute_reply":"2023-11-19T07:50:22.193274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotCorrelationMatrix(df2, 8)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:22.920616Z","iopub.execute_input":"2023-11-19T07:50:22.921317Z","iopub.status.idle":"2023-11-19T07:50:22.935248Z","shell.execute_reply.started":"2023-11-19T07:50:22.921244Z","shell.execute_reply":"2023-11-19T07:50:22.933844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotScatterMatrix(df2, 6, 15)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:23.293416Z","iopub.execute_input":"2023-11-19T07:50:23.294097Z","iopub.status.idle":"2023-11-19T07:50:23.681487Z","shell.execute_reply.started":"2023-11-19T07:50:23.294017Z","shell.execute_reply":"2023-11-19T07:50:23.679693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's check 3rd file: /kaggle/input/NY_Restauraunts_tips.csv","metadata":{}},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# NY_Restauraunts_tips.csv has 10377 rows in reality, but we are only loading/previewing the first 1000 rows\ndf3 = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1',nrows = nRowsRead)\ndf3.dataframeName = 'NY_Restauraunts_tips.csv'\nnRow, nCol = df3.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:24.937549Z","iopub.execute_input":"2023-11-19T07:50:24.937979Z","iopub.status.idle":"2023-11-19T07:50:24.964579Z","shell.execute_reply.started":"2023-11-19T07:50:24.937915Z","shell.execute_reply":"2023-11-19T07:50:24.963158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:","metadata":{}},{"cell_type":"code","source":"df3.head(5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:26.299834Z","iopub.execute_input":"2023-11-19T07:50:26.300589Z","iopub.status.idle":"2023-11-19T07:50:26.313790Z","shell.execute_reply.started":"2023-11-19T07:50:26.300501Z","shell.execute_reply":"2023-11-19T07:50:26.312561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:","metadata":{}},{"cell_type":"code","source":"plotPerColumnDistribution(df3, 10, 5)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:28.212586Z","iopub.execute_input":"2023-11-19T07:50:28.213030Z","iopub.status.idle":"2023-11-19T07:50:28.232698Z","shell.execute_reply.started":"2023-11-19T07:50:28.212969Z","shell.execute_reply":"2023-11-19T07:50:28.231468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation matrix:","metadata":{}},{"cell_type":"code","source":"plotCorrelationMatrix(df3, 8)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:29.401875Z","iopub.execute_input":"2023-11-19T07:50:29.402397Z","iopub.status.idle":"2023-11-19T07:50:29.793480Z","shell.execute_reply.started":"2023-11-19T07:50:29.402317Z","shell.execute_reply":"2023-11-19T07:50:29.791833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatter and density plots:","metadata":{}},{"cell_type":"code","source":"plotScatterMatrix(df3, 6, 15)","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-19T07:50:30.551719Z","iopub.execute_input":"2023-11-19T07:50:30.552487Z","iopub.status.idle":"2023-11-19T07:50:31.472554Z","shell.execute_reply.started":"2023-11-19T07:50:30.552404Z","shell.execute_reply":"2023-11-19T07:50:31.470565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","metadata":{}},{"cell_type":"code","source":"print(df1.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:31.980964Z","iopub.execute_input":"2023-11-19T07:50:31.981415Z","iopub.status.idle":"2023-11-19T07:50:31.988458Z","shell.execute_reply.started":"2023-11-19T07:50:31.981360Z","shell.execute_reply":"2023-11-19T07:50:31.987098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df2.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:32.766251Z","iopub.execute_input":"2023-11-19T07:50:32.766947Z","iopub.status.idle":"2023-11-19T07:50:32.774162Z","shell.execute_reply.started":"2023-11-19T07:50:32.766809Z","shell.execute_reply":"2023-11-19T07:50:32.772668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df3.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:33.476858Z","iopub.execute_input":"2023-11-19T07:50:33.477332Z","iopub.status.idle":"2023-11-19T07:50:33.484093Z","shell.execute_reply.started":"2023-11-19T07:50:33.477263Z","shell.execute_reply":"2023-11-19T07:50:33.483003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming the actual column name is different, replace 'checkins' with the correct column name\ndf1['score'] = df1['venue_ID']\n\n# Alternatively, you can use other aggregation functions like mean or sum, depending on your specific goal\n# For example, calculating the mean check-ins per restaurant:\n# df1['score'] = df1.groupby('restaurant_id')['actual_column_name'].transform('mean')\n\n# Display the updated DataFrame\nprint(df1[['user_ID', 'venue_ID', 'score']].head())\n\n# Plot the distribution of the calculated scores\nplotPerColumnDistribution(df1[['score']], 10, 1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:33.931960Z","iopub.execute_input":"2023-11-19T07:50:33.932438Z","iopub.status.idle":"2023-11-19T07:50:33.957321Z","shell.execute_reply.started":"2023-11-19T07:50:33.932357Z","shell.execute_reply":"2023-11-19T07:50:33.955764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Common Neighbors\nplt.figure(figsize=(10, 6))\nplt.scatter(df_common_neighbors['common_neighbors_column'], df_common_neighbors['preferential_attachment_column'], alpha=0.5)\nplt.title('Common Neighbors vs Preferential Attachment')\nplt.xlabel('Common Neighbors')\nplt.ylabel('Preferential Attachment')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:34.710974Z","iopub.execute_input":"2023-11-19T07:50:34.711394Z","iopub.status.idle":"2023-11-19T07:50:34.780374Z","shell.execute_reply.started":"2023-11-19T07:50:34.711334Z","shell.execute_reply":"2023-11-19T07:50:34.778666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print column names of df_common_neighbors\nprint(df_common_neighbors.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:39:27.412513Z","iopub.execute_input":"2023-11-19T07:39:27.412968Z","iopub.status.idle":"2023-11-19T07:39:27.430984Z","shell.execute_reply.started":"2023-11-19T07:39:27.412906Z","shell.execute_reply":"2023-11-19T07:39:27.429220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Preferential Attachment for each edge\npreferential_attachment = [(u, v, G.degree(u) * G.degree(v)) for u, v in G.edges()]\ndf_preferential_attachment = pd.DataFrame(preferential_attachment, columns=['venue1', 'venue2', 'preferential_attachment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:42.138164Z","iopub.execute_input":"2023-11-19T07:50:42.138609Z","iopub.status.idle":"2023-11-19T07:50:42.161433Z","shell.execute_reply.started":"2023-11-19T07:50:42.138545Z","shell.execute_reply":"2023-11-19T07:50:42.159980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Common Neighbors vs Preferential Attachment\nplt.figure(figsize=(10, 6))\nplt.scatter(df_common_neighbors['common_neighbors'], df_preferential_attachment['preferential_attachment'], alpha=0.5)\nplt.title('Common Neighbors vs Preferential Attachment')\nplt.xlabel('Common Neighbors')\nplt.ylabel('Preferential Attachment')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:42.709462Z","iopub.execute_input":"2023-11-19T07:50:42.709899Z","iopub.status.idle":"2023-11-19T07:50:42.737438Z","shell.execute_reply.started":"2023-11-19T07:50:42.709839Z","shell.execute_reply":"2023-11-19T07:50:42.735869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Assuming df_common_neighbors, df_preferential_attachment, clustering_coefficient, degree_centrality, communities, and pagerank are calculated\n\n# Plot Common Neighbors vs Preferential Attachment\nplt.figure(figsize=(10, 6))\nplt.scatter(df_preferential_attachment['venue1'], df_preferential_attachment['preferential_attachment'], alpha=0.5)\nplt.title('Common Neighbors vs Preferential Attachment')\nplt.xlabel('Common Neighbors')\nplt.ylabel('Preferential Attachment')\nplt.grid(True)\nplt.show()\n\n# Plot Clustering Coefficient\nplt.figure(figsize=(10, 6))\nplt.bar(clustering_coefficient.keys(), clustering_coefficient.values())\nplt.title('Clustering Coefficient for Venues')\nplt.xlabel('Venue ID')\nplt.ylabel('Clustering Coefficient')\nplt.xticks(rotation=90)\nplt.show()\n\n# Plot Degree Centrality\nplt.figure(figsize=(10, 6))\nplt.bar(degree_centrality.keys(), degree_centrality.values())\nplt.title('Degree Centrality for Venues')\nplt.xlabel('Venue ID')\nplt.ylabel('Degree Centrality')\nplt.xticks(rotation=90)\nplt.show()\n\n# Plot Community Detection\ncolors = ['red', 'green', 'blue', 'yellow', 'purple']  # Add more colors if needed\nnode_color = [colors[i] for i, community in enumerate(communities) for _ in community]\n\nplt.figure(figsize=(12, 8))\npos = nx.spring_layout(G)\nnx.draw(G, pos, node_color=node_color, with_labels=True)\nplt.title('Community Detection')\nplt.show()\n\n# Plot PageRank scores\n# Plot Common Neighbors vs Preferential Attachment\nplt.figure(figsize=(10, 6))\nplt.scatter(df_preferential_attachment['venue1'], df_preferential_attachment['preferential_attachment'], alpha=0.5)\nplt.title('Common Neighbors vs Preferential Attachment')\nplt.xlabel('Common Neighbors')\nplt.ylabel('Preferential Attachment')\nplt.grid(True)\nplt.show()\n\n# Plot Community Detection\nnum_communities = len(communities)\ncolors = plt.cm.viridis.colors  # You can choose a different colormap if needed\n\nnode_color = [colors[i % len(colors)] for i, community in enumerate(communities) for _ in community]\n\nplt.figure(figsize=(12, 8))\npos = nx.spring_layout(G)\nnx.draw(G, pos, node_color=node_color, with_labels=True)\nplt.title('Community Detection')\nplt.show()\n\n# Link Prediction visualization may vary based on the specific algorithm used\n# Additional visualizations can be added based on your requirements\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:43.273307Z","iopub.execute_input":"2023-11-19T07:50:43.273718Z","iopub.status.idle":"2023-11-19T07:50:43.349406Z","shell.execute_reply.started":"2023-11-19T07:50:43.273671Z","shell.execute_reply":"2023-11-19T07:50:43.347889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install matplotlib networkx python-louvain\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:50:44.252098Z","iopub.execute_input":"2023-11-19T07:50:44.252602Z","iopub.status.idle":"2023-11-19T07:51:12.449469Z","shell.execute_reply.started":"2023-11-19T07:50:44.252517Z","shell.execute_reply":"2023-11-19T07:51:12.448042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming checkins_data is a DataFrame with columns: 'user_id', 'venue_id', 'timestamp'\ncheckins_data['timestamp'] = pd.to_datetime(checkins_data['timestamp'])\n\n# Extract temporal features\ncheckins_data['hour'] = checkins_data['timestamp'].dt.hour\ncheckins_data['day_of_week'] = checkins_data['timestamp'].dt.dayofweek\ncheckins_data['month'] = checkins_data['timestamp'].dt.month\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:51:12.452097Z","iopub.execute_input":"2023-11-19T07:51:12.452455Z","iopub.status.idle":"2023-11-19T07:51:12.481780Z","shell.execute_reply.started":"2023-11-19T07:51:12.452393Z","shell.execute_reply":"2023-11-19T07:51:12.479998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['text.usetex'] = False\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.240166Z","iopub.status.idle":"2023-11-19T07:37:26.240694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntext_to_display = \"waiters!!!,socialite, exclusive, $$$$, jetsetter,sushi,zagat rated\"\n\nfig, ax = plt.subplots()\nax.annotate(text_to_display, xy=(0.5, 0.5), xycoords='axes fraction',\n            fontsize=12, ha='center', va='center', fontfamily='monospace')\n\n# Save the figure or display it as needed\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.241876Z","iopub.status.idle":"2023-11-19T07:37:26.242368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom wordcloud import WordCloud\nimport networkx as nx\nimport matplotlib.pyplot as plt\nplt.rcParams['text.usetex'] = False\n# Load data\ndf_checkins = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ndf_tips = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ndf_tags = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n\n# Create a graph using NetworkX\nG = nx.Graph()\n\n# Add edges for check-ins\nG.add_edges_from(zip(df_checkins['user_ID'], df_checkins['venue_ID']))\n\n# Add edges for tips\nG.add_edges_from(zip(df_tips['user_ID'], df_tips['venue_ID']))\n\n# Add edges for tags\nG.add_edges_from(zip(df_tags['venue_ID'], df_tags['tags']))\n\n# Visualize the graph\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, font_weight='bold')\nplt.title(\"User-Venue Interaction Network\")\nplt.show()\n\n# Group by venue and concatenate tags\nvenue_tags = df_tags.groupby('venue_ID')['tags'].apply(lambda x: ' '.join(str(i) for i in x)).reset_index()\n\n# Merge with check-ins and tips data to include user-venue interactions\nvenue_tags = pd.merge(venue_tags, df_checkins, on='venue_ID', how='left')\nvenue_tags = pd.merge(venue_tags, df_tips, on='venue_ID', how='left')\n\n# Plot tag distribution\ntag_counts = venue_tags['tags'].str.split().explode().value_counts()\ntag_counts.plot(kind='bar', figsize=(10, 6), color='skyblue')\nplt.title(\"Tag Distribution for Venues\")\nplt.xlabel(\"Tag\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:39:45.490663Z","iopub.execute_input":"2023-11-19T07:39:45.491140Z","iopub.status.idle":"2023-11-19T07:40:44.687169Z","shell.execute_reply.started":"2023-11-19T07:39:45.491056Z","shell.execute_reply":"2023-11-19T07:40:44.686194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Load datasets\ncheckins_df = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntips_df = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntags_df = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes and edges from checkins\nG.add_nodes_from(checkins_df['user_ID'], bipartite=0)\nG.add_nodes_from(checkins_df['venue_ID'], bipartite=1)\nG.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add edges from tips\nG.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Visualize the graph\npos = nx.spring_layout(G)\nplt.figure(figsize=(10, 8))\nnx.draw(G, pos, node_size=10)\nplt.title('Restaurant Check-ins and Tips Network')\nplt.show()\n\n# Analysis\nprint(\"Number of nodes:\", G.number_of_nodes())\nprint(\"Number of edges:\", G.number_of_edges())\nprint(\"Density of the graph:\", nx.density(G))\n\n# Perform link prediction algorithms (e.g., Common Neighbors, Jaccard Coefficient, Adamic-Adar Index)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:33:24.911226Z","iopub.execute_input":"2023-11-19T08:33:24.911664Z","iopub.status.idle":"2023-11-19T08:33:59.412796Z","shell.execute_reply.started":"2023-11-19T08:33:24.911589Z","shell.execute_reply":"2023-11-19T08:33:59.411716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Load check-ins dataset\ncheckins_df = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Create a bipartite graph\nG = nx.Graph()\nG.add_nodes_from(checkins_df['user_ID'], bipartite=0)\nG.add_nodes_from(checkins_df['venue_ID'], bipartite=1)\nG.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Plot the degree distribution of user nodes\nuser_degrees = [deg for node, deg in G.degree() if G.nodes[node]['bipartite'] == 0]\nplt.hist(user_degrees, bins=50, alpha=0.5, color='b', label='Users')\nplt.title('Degree Distribution of User Nodes')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n# Plot the degree distribution of venue nodes\nvenue_degrees = [deg for node, deg in G.degree() if G.nodes[node]['bipartite'] == 1]\nplt.hist(venue_degrees, bins=50, alpha=0.5, color='r', label='Venues')\nplt.title('Degree Distribution of Venue Nodes')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n# Identify the largest weakly connected component\nlargest_wcc = max(nx.connected_components(G), key=len)\nG_largest_wcc = G.subgraph(largest_wcc)\n\n# Set a degree threshold (replace with your desired threshold)\ndegree_threshold = 2\n\n# Remove nodes with a degree less than the threshold\nfiltered_nodes = [node for node in G_largest_wcc.nodes if G_largest_wcc.degree(node) >= degree_threshold]\nG_filtered = G_largest_wcc.subgraph(filtered_nodes)\n\n# Plot the degree distribution of filtered user nodes\nuser_degrees_filtered = [deg for node, deg in G_filtered.degree() if G_filtered.nodes[node]['bipartite'] == 0]\nplt.hist(user_degrees_filtered, bins=50, alpha=0.5, color='b', label='Users')\nplt.title('Filtered Degree Distribution of User Nodes')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n# Plot the degree distribution of filtered venue nodes\nvenue_degrees_filtered = [deg for node, deg in G_filtered.degree() if G_filtered.nodes[node]['bipartite'] == 1]\nplt.hist(venue_degrees_filtered, bins=50, alpha=0.5, color='r', label='Venues')\nplt.title('Filtered Degree Distribution of Venue Nodes')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:41:27.281349Z","iopub.execute_input":"2023-11-19T07:41:27.281689Z","iopub.status.idle":"2023-11-19T07:41:29.255650Z","shell.execute_reply.started":"2023-11-19T07:41:27.281619Z","shell.execute_reply":"2023-11-19T07:41:29.254121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\n# Load your dataset\n# Assume you have loaded NY_Restauraunts_checkins.csv, NY_Restauraunts_tips.csv, NY_Restauraunts_tags.csv\ncheckins_data = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntips_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntags_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Merge dataframes based on venue_id\nmerged_data = pd.merge(checkins_data, tips_data, on=['user_ID', 'venue_ID'], how='outer')\nmerged_data = pd.merge(merged_data, tags_data, on='venue_ID', how='outer')\n\n# Assuming you have a target variable (e.g., whether a user liked the venue or not)\n# You can create a binary target variable based on your specific problem\n\n# For example, let's assume the target is binary: 1 if the user liked the venue, 0 otherwise\nmerged_data['liked'] = merged_data['tips'].notnull().astype(int)\n\n# Drop unnecessary columns\nX = merged_data.drop(['user_ID', 'venue_ID', 'tips', 'tags', 'liked'], axis=1)\ny = merged_data['liked']\n\n# Convert text data to numerical representation using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\nX_tfidf = tfidf_vectorizer.fit_transform(merged_data['tips'].fillna(''))\n\n# Combine TF-IDF features with other features\nX_combined = pd.concat([X, pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())], axis=1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n\n# Initialize the kNN classifier\nknn = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors\n\n# Train the model\nknn.fit(X_train, y_train)\n\n# Make predictions\ny_pred = knn.predict(X_test)\n\n# Calculate precision and recall\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print the results\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:41:29.258448Z","iopub.execute_input":"2023-11-19T07:41:29.259612Z","iopub.status.idle":"2023-11-19T07:41:37.917276Z","shell.execute_reply.started":"2023-11-19T07:41:29.259518Z","shell.execute_reply":"2023-11-19T07:41:37.916189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_score, recall_score\n\n# Load datasets\ncheckins = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1')\ntips = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1')\ntags = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1')\n\n# Merge datasets\nmerged_data = pd.merge(checkins, tips, on=['user_ID', 'venue_ID'], how='outer')\nmerged_data = pd.merge(merged_data, tags, on='venue_ID', how='outer')\n\n# Create feature vectors\nfeatures = merged_data.groupby('venue_ID').size().reset_index(name='checkin_count')\n\n# Split dataset into training and testing sets\ntrain, test = train_test_split(features, test_size=0.2, random_state=42)\n\n# Fit k-NN model\nknn = NearestNeighbors(n_neighbors=5)  # You can adjust the number of neighbors\nknn.fit(train[['checkin_count']])\n\n# Find nearest neighbors for test instances\ndistances, indices = knn.kneighbors(test[['checkin_count']])\n\n# Evaluate precision and recall using ROC curve\n# Assuming a binary classification problem (e.g., recommending or not recommending a venue)\n# You need to define your own threshold for recommendation based on the problem requirements\n\n# Example threshold, adjust according to your problem\nthreshold = 55\n\n\n# Convert distances to binary predictions\npredictions = (distances < threshold).astype(int)\n\n# Calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(test['checkin_count'] >= threshold, predictions.sum(axis=1) > 0)\n\n# Choose a higher precision threshold\ntarget_precision = 0.3  # Adjust as needed\n\n# Find the index of the closest precision to the target\nbest_threshold_index = next((i for i, p in enumerate(precision) if p >= target_precision), len(precision) - 1)\n\n# Use the threshold that achieves the desired precision or the maximum available threshold\nbest_threshold = thresholds[best_threshold_index] if best_threshold_index < len(thresholds) else thresholds[-1]\n\n# Recalculate precision and recall with the best threshold\nupdated_predictions = (distances < best_threshold).astype(int)\nupdated_precision = precision_score(test['checkin_count'] >= threshold, updated_predictions.sum(axis=1) > 0)\nupdated_recall = recall_score(test['checkin_count'] >= threshold, updated_predictions.sum(axis=1) > 0)\n\nprint(f'Updated Precision: {updated_precision:.4f}')\nprint(f'Updated Recall: {updated_recall:.4f}')\nprint(f'Used Threshold: {best_threshold:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:41:37.920746Z","iopub.execute_input":"2023-11-19T07:41:37.921183Z","iopub.status.idle":"2023-11-19T07:41:38.070958Z","shell.execute_reply.started":"2023-11-19T07:41:37.921100Z","shell.execute_reply":"2023-11-19T07:41:38.069853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"recall = tpr.sum()\nprecision = tpr.sum() / (tpr.sum() + fpr.sum())\n\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:46:27.122525Z","iopub.execute_input":"2023-11-18T12:46:27.123283Z","iopub.status.idle":"2023-11-18T12:46:27.130467Z","shell.execute_reply.started":"2023-11-18T12:46:27.123210Z","shell.execute_reply":"2023-11-18T12:46:27.129537Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport pandas as pd\n\n# Load your dataset\n# Assume you have loaded NY_Restaurants_checkins.csv, NY_Restaurants_tips.csv, NY_Restaurants_tags.csv\ncheckins_data = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntips_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntags_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Merge dataframes based on common columns\n# Convert 'user_ID' and 'venue_ID' columns to strings\nmerged_data['user_ID'] = merged_data['user_ID'].astype(str)\nmerged_data['venue_ID'] = merged_data['venue_ID'].astype(str)\n\n# Concatenate 'user_ID' and 'venue_ID' with an underscore\ny = merged_data['user_ID'] + '_' + merged_data['venue_ID']\n\n# Fill NaN values with appropriate placeholders or drop them based on your requirement\n# ...\n\n# Convert 'venue_ID' column to strings\nmerged_data['venue_ID'] = merged_data['venue_ID'].astype(str)\n\n# Handle NaN values in 'tips' column\nmerged_data['tips'] = merged_data['tips'].fillna('')  # Replace NaN with an empty string\n\n# Feature extraction\nvectorizer = TfidfVectorizer(stop_words='english', min_df=2)  # Ignore common English stop words and consider words that appear in at least 2 documents\nmlb = MultiLabelBinarizer()\n\nX_text = vectorizer.fit_transform(merged_data['tips'])\nX_tags = mlb.fit_transform(merged_data['tips'].apply(lambda x: x.split(',')))\n\n# Concatenate text features and tag features\nX = pd.concat([pd.DataFrame(X_text.toarray()), pd.DataFrame(X_tags)], axis=1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Make predictions\ny_pred = knn.predict(X_test)\n\n# Calculate precision and recall\nprecision = precision_score(y_test, y_pred, average='micro')\nrecall = recall_score(y_test, y_pred, average='micro')\n\n# Print the results\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:41:38.072992Z","iopub.execute_input":"2023-11-19T07:41:38.073530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display unique values in the 'tips' column\nprint(\"Unique values in 'tips':\", merged_data['tips'].unique())\n\n# Display some sample 'tips' entries\nprint(\"Sample 'tips' entries:\")\nfor tip in merged_data['tips'][:10]:\n    print(tip)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries for KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (n_neighbors) based on your preference\n\n# Train the KNN classifier\nknn.fit(X_train, y_train)\n\n# Make predictions\ny_pred_knn = knn.predict(X_test)\n\n# Evaluate the KNN model\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nprecision_knn = precision_score(y_test, y_pred_knn, average='micro')\nrecall_knn = recall_score(y_test, y_pred_knn, average='micro')\n\n# Print the results for KNN\nprint('Results for KNN:')\nprint(f'Accuracy: {accuracy_knn}')\nprint(f'Precision: {precision_knn}')\nprint(f'Recall: {recall_knn}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.254735Z","iopub.status.idle":"2023-11-19T07:37:26.255306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport pandas as pd\n\n# Load your dataset\n# Assume you have loaded NY_Restauraunts_checkins.csv, NY_Restauraunts_tips.csv, NY_Restauraunts_tags.csv\ncheckins_data = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntips_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntags_data = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Merge dataframes based on common columns\n# Display column names before merging\nprint(\"Columns in checkins_data:\", checkins_data.columns)\nprint(\"Columns in tips_data:\", tips_data.columns)\n\n# Merge dataframes based on common columns\nmerged_data = pd.merge(checkins_data, tips_data, on=['user_ID', 'venue_ID'], how='inner')\n\n# Display column names after merging\nprint(\"Columns in merged_data:\", merged_data.columns)\n\n# Check the column names in merged_data\nprint(merged_data.columns)\n\n# Ensure 'tags' column is present in merged_data before proceeding\nif 'tips' not in merged_data.columns:\n    raise KeyError(\"'tags' column not found in merged_data\")\n\n# Convert 'user_ID' and 'venue_ID' columns to strings\nmerged_data['user_ID'] = merged_data['user_ID'].astype(str)\nmerged_data['venue_ID'] = merged_data['venue_ID'].astype(str)\n\n# Concatenate 'user_ID' and 'venue_ID' with an underscore\ny = merged_data['user_ID'] + '_' + merged_data['venue_ID']\n\n# Convert 'venue_ID' column to strings\nmerged_data['venue_ID'] = merged_data['venue_ID'].astype(str)\n\n# Handle NaN values in 'tags' column\nmerged_data['tips'] = merged_data['tips'].fillna('')  # Replace NaN with an empty string\n\n# Feature extraction\nvectorizer = TfidfVectorizer()\nmlb = MultiLabelBinarizer()\n\nX_text = vectorizer.fit_transform(merged_data['venue_ID'])\nX_tags = mlb.fit_transform(merged_data['tips'].apply(lambda x: x.split(',')))\n\n# Concatenate text features and tag features\nX = pd.concat([pd.DataFrame(X_text.toarray()), pd.DataFrame(X_tags)], axis=1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Support Vector Machine (SVM) classifier\nclf = SVC(kernel='linear', C=1.0, random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='micro')\nrecall = recall_score(y_test, y_pred, average='micro')\n\n# Print the results\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.256347Z","iopub.status.idle":"2023-11-19T07:37:26.256945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ...\n\n# Train a Support Vector Machine (SVM) classifier\nclf = SVC(kernel='linear', C=1.0, random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision_micro = precision_score(y_test, y_pred, average='micro')\nrecall_micro = recall_score(y_test, y_pred, average='micro')\nprecision_macro = precision_score(y_test, y_pred, average='macro')\nrecall_macro = recall_score(y_test, y_pred, average='macro')\nprecision_weighted = precision_score(y_test, y_pred, average='weighted')\nrecall_weighted = recall_score(y_test, y_pred, average='weighted')\n\n# Print the results\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision (Micro): {precision_micro}')\nprint(f'Recall (Micro): {recall_micro}')\nprint(f'Precision (Macro): {precision_macro}')\nprint(f'Recall (Macro): {recall_macro}')\nprint(f'Precision (Weighted): {precision_weighted}')\nprint(f'Recall (Weighted): {recall_weighted}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.257978Z","iopub.status.idle":"2023-11-19T07:37:26.258562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef top_k_accuracy_score(y_true, y_score, k=5):\n    top_k_predictions = np.argsort(y_score, axis=1)[:, -k:]\n    true_in_top_k = np.any(top_k_predictions == y_true.reshape(-1, 1), axis=1)\n    top_k_accuracy = np.mean(true_in_top_k)\n    return top_k_accuracy\n\n# Usage\nk = 5\ntop_k_accuracy = top_k_accuracy_score(y_test, y_pred, k=k)\nprint(f'Top-{k} Accuracy: {top_k_accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.259576Z","iopub.status.idle":"2023-11-19T07:37:26.260178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\n# Assuming you have trained your classifier and loaded your test data (X_test)\n# ...\n\n# List of similarity metrics\nsimilarity_metrics = ['Random Link', 'Distance', 'Common Neighbor (User)', \n                      'Common Neighbor (Venue)', 'Preferential Attachment', \n                      'Adamic-Adar (User)', 'Adamic-Adar (Venue)', 'Katz']\n\n# Results containers\nprecision_values = []\nrecall_values = []\n\n# Loop through each similarity metric\nfor metric in similarity_metrics:\n    # Check if the metric is present in any part of the venue IDs\n    y_test_metric = y_test.apply(lambda x: metric in x)\n    \n    # If at least one instance of the metric is present, calculate precision and recall\n    if y_test_metric.any():\n        y_pred_metric = y_pred[y_test_metric.index]\n\n        # Calculate precision and recall based on your predictions and ground truth\n        precision = precision_score(y_test_metric, y_pred_metric, average='binary')\n        recall = recall_score(y_test_metric, y_pred_metric, average='binary')\n    else:\n        # If the metric is not present in any venue ID, set precision and recall to 0\n        precision, recall = 0.0, 0.0\n\n    # Append results to the lists\n    precision_values.append(precision)\n    recall_values.append(recall)\n\n# Create a DataFrame to display the results\nresults_df = pd.DataFrame({\n    'Similarity Metric': similarity_metrics,\n    'Precision': precision_values,\n    'Recall': recall_values\n})\n\n# Print the results\nprint(results_df)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.261221Z","iopub.status.idle":"2023-11-19T07:37:26.261818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Print the unique values in y_test\nprint(\"Unique values in y_test:\", len(np.unique(y_test)))\n\n# Print the unique values in y_pred\nprint(\"Unique values in y_pred:\", len(np.unique(y_pred)))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.262883Z","iopub.status.idle":"2023-11-19T07:37:26.263450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example: Comparison for 'Random Link'\nmetric = 'Random Link'\nprecision = precision_score(y_test == metric, y_pred == metric, average='binary')\nrecall = recall_score(y_test == metric, y_pred == metric, average='binary')\nprint(f'Precision for {metric}: {precision}')\nprint(f'Recall for {metric}: {recall}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.264482Z","iopub.status.idle":"2023-11-19T07:37:26.265070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ...\n\n# Replace the following placeholders with your actual link prediction algorithms\n\n# Random Link\ndef random_link_prediction(X_test):\n    # Implement or use a function to predict links using the Random Link algorithm\n    # Replace 'random_predictions' with the actual predictions\n    random_predictions = ...\n    return random_predictions\n\n# Distance\ndef distance_prediction(X_test):\n    # Implement or use a function to predict links using the Distance algorithm\n    # Replace 'distance_predictions' with the actual predictions\n    distance_predictions = ...\n    return distance_predictions\n\n# Common Neighbor (User)\ndef common_neighbor_user_prediction(X_test):\n    # Implement or use a function to predict links using the Common Neighbor (User) algorithm\n    # Replace 'common_neighbor_user_predictions' with the actual predictions\n    common_neighbor_user_predictions = ...\n    return common_neighbor_user_predictions\n\n# Common Neighbor (Venue)\ndef common_neighbor_venue_prediction(X_test):\n    # Implement or use a function to predict links using the Common Neighbor (Venue) algorithm\n    # Replace 'common_neighbor_venue_predictions' with the actual predictions\n    common_neighbor_venue_predictions = ...\n    return common_neighbor_venue_predictions\n\n# Preferential Attachment\ndef preferential_attachment_prediction(X_test):\n    # Implement or use a function to predict links using the Preferential Attachment algorithm\n    # Replace 'preferential_attachment_predictions' with the actual predictions\n    preferential_attachment_predictions = ...\n    return preferential_attachment_predictions\n\n# Adamic-Adar (User)\ndef adamic_adar_user_prediction(X_test):\n    # Implement or use a function to predict links using the Adamic-Adar (User) algorithm\n    # Replace 'adamic_adar_user_predictions' with the actual predictions\n    adamic_adar_user_predictions = ...\n    return adamic_adar_user_predictions\n\n# Adamic-Adar (Venue)\ndef adamic_adar_venue_prediction(X_test):\n    # Implement or use a function to predict links using the Adamic-Adar (Venue) algorithm\n    # Replace 'adamic_adar_venue_predictions' with the actual predictions\n    adamic_adar_venue_predictions = ...\n    return adamic_adar_venue_predictions\n\n# Katz\ndef katz_prediction(X_test):\n    # Implement or use a function to predict links using the Katz algorithm\n    # Replace 'katz_predictions' with the actual predictions\n    katz_predictions = ...\n    return katz_predictions\n\n# ...\n\n# Calculate precision and recall for each link prediction algorithm\n\nprecision_micro_random = precision_score(y_test, random_link_prediction(X_test), average='micro')\nrecall_micro_random = recall_score(y_test, random_link_prediction(X_test), average='micro')\nprecision_macro_random = precision_score(y_test, random_link_prediction(X_test), average='macro')\nrecall_macro_random = recall_score(y_test, random_link_prediction(X_test), average='macro')\nprecision_weighted_random = precision_score(y_test, random_link_prediction(X_test), average='weighted')\nrecall_weighted_random = recall_score(y_test, random_link_prediction(X_test), average='weighted')\n\n# Repeat the above code for each link prediction algorithm\n# ...\n\n# Print results for each link prediction algorithm\n\nprint(\"\\nRandom Link Results:\")\nprint(f'Precision (Micro): {precision_micro_random}')\nprint(f'Recall (Micro): {recall_micro_random}')\nprint(f'Precision (Macro): {precision_macro_random}')\nprint(f'Recall (Macro): {recall_macro_random}')\nprint(f'Precision (Weighted): {precision_weighted_random}')\nprint(f'Recall (Weighted): {recall_weighted_random}')\n\n# Repeat the above print statements for each link prediction algorithm\n# ...\n\n# ...\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.266140Z","iopub.status.idle":"2023-11-19T07:37:26.266740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load datasets\ncheckins_df = pd.read_csv('/kaggle/input/NY_Restauraunts_checkins.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntips_df = pd.read_csv('/kaggle/input/NY_Restauraunts_tips.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\ntags_df = pd.read_csv('/kaggle/input/NY_Restauraunts_tags.csv', delimiter=',', encoding='ISO-8859-1', nrows=nRowsRead)\n\n# Create a graph\n# G = nx.Graph()\n\n# # Add edges from check-ins\n# checkin_edges = [(row['user_ID'], row['venue_ID']) for index, row in checkins_df.iterrows()]\n# G.add_edges_from(checkin_edges)\n\n# # Add edges from tips (optional, depending on your analysis)\n# tip_edges = [(row['user_ID'], row['venue_ID']) for index, row in tips_df.iterrows()]\n# G.add_edges_from(tip_edges)\n\n# # Separate users and venues for plotting\n# user_nodes = {n for n, d in G.nodes(data=True) if 'user_ID' in d}\n# venue_nodes = set(G) - user_nodes\n# Create a bipartite graph\nG = nx.Graph()\n\n# Add user nodes with 'user_id' attribute\nG.add_nodes_from([(str(user), {'user_ID': str(user)}) for user in checkins_df['user_ID']])\n\n# Add venue nodes with 'venue_id' attribute\nG.add_nodes_from([(str(venue), {'venue_ID': str(venue)}) for venue in checkins_df['venue_ID']])\n\n# Add edges from check-ins\ncheckin_edges = [(str(row['user_ID']), str(row['venue_ID'])) for _, row in checkins_df.iterrows()]\nG.add_edges_from(checkin_edges)\n\n# Separate users and venues for scoring\nuser_nodes = {n for n, d in G.nodes(data=True) if 'user_ID' in d}\nvenue_nodes = set(G) - user_nodes\n\n\n# Plot bipartite graph\npos = {node: (0, i) for i, node in enumerate(user_nodes)}\npos.update({node: (1, i) for i, node in enumerate(venue_nodes)})\nnx.draw(G, pos=pos, font_weight='bold')\nplt.title('Bipartite Graph of Users and Venues')\nplt.show()\n\n# Plot degree distribution\ndegree_sequence = [d for n, d in G.degree()]\nplt.hist(degree_sequence, bins=20, alpha=0.75)\nplt.title(\"Degree Distribution\")\nplt.xlabel(\"Degree\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Calculate node centralities\ncentrality = nx.degree_centrality(G)\n\n# Plot nodes with sizes proportional to centrality\nnode_sizes = [centrality[node] * 500 for node in G.nodes()]\nnx.draw(G, with_labels=True, node_size=node_sizes, cmap=plt.cm.Blues, font_weight='bold')\nplt.title('Network Centrality Visualization')\nplt.show()\n\n# Detect communities using Louvain algorithm\npartition = community.best_partition(G)\n\n# Plot the graph with communities in different colors\npos = nx.spring_layout(G)  # You can use other layouts\ncmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\nnx.draw_networkx_nodes(G, pos, node_size=50, cmap=cmap, node_color=list(partition.values()))\nnx.draw_networkx_edges(G, pos, alpha=0.5)\nplt.title('Community Detection Visualization')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:51:55.834265Z","iopub.execute_input":"2023-11-19T07:51:55.834993Z","iopub.status.idle":"2023-11-19T07:52:14.953850Z","shell.execute_reply.started":"2023-11-19T07:51:55.834912Z","shell.execute_reply":"2023-11-19T07:52:14.952076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\nB = nx.Graph()\n\n# Add nodes and edges for check-ins\nB.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add nodes and edges for tips\nB.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Add nodes for tags\nB.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Separate nodes by bipartite attribute for coloring\nuser_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 0]\nvenue_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 1]\n\n# Draw the bipartite graph with different colors for users and venues\npos = nx.spring_layout(B)\nnx.draw_networkx_nodes(B, pos, nodelist=user_nodes, node_color='skyblue', node_size=50, label='Users')\nnx.draw_networkx_nodes(B, pos, nodelist=venue_nodes, node_color='salmon', node_size=50, label='Venues')\nnx.draw_networkx_edges(B, pos, width=1.0, alpha=0.5)\n# nx.draw_networkx_labels(B, pos)\nplt.title('Bipartite Graph of NYC Restaurants')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:52:23.690742Z","iopub.execute_input":"2023-11-19T07:52:23.691190Z","iopub.status.idle":"2023-11-19T07:52:54.687712Z","shell.execute_reply.started":"2023-11-19T07:52:23.691112Z","shell.execute_reply":"2023-11-19T07:52:54.686603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\nB = nx.Graph()\n\n# Add nodes and edges for check-ins\nB.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add nodes and edges for tips\nB.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Add nodes for tags\nB.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Manually set positions for the bipartite graph\nuser_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 0]\nvenue_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 1]\n\npos = {}\npos.update((node, (1, index)) for index, node in enumerate(user_nodes))  # Users on the left\npos.update((node, (2, index)) for index, node in enumerate(venue_nodes))  # Venues on the right\n\n# Draw the bipartite graph with different colors for users and venues\nnx.draw_networkx_nodes(B, pos, nodelist=user_nodes, node_color='skyblue', node_size=50, label='Users')\nnx.draw_networkx_nodes(B, pos, nodelist=venue_nodes, node_color='salmon', node_size=50, label='Venues')\nnx.draw_networkx_edges(B, pos, width=1.0, alpha=0.5)\n# nx.draw_networkx_labels(B, pos)\nplt.title('Bipartite Graph of NYC Restaurants')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:52:54.690230Z","iopub.execute_input":"2023-11-19T07:52:54.690554Z","iopub.status.idle":"2023-11-19T07:52:55.498045Z","shell.execute_reply.started":"2023-11-19T07:52:54.690501Z","shell.execute_reply":"2023-11-19T07:52:55.496205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport community  # Louvain community detection\n\n# Assuming you have loaded the datasets and created the bipartite graph 'B'\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\n# B = nx.Graph()\n\n# # Add nodes and edges for check-ins\n# B.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\n# B.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\n# B.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# # Add nodes and edges for tips\n# B.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\n# B.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\n# B.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# # Add nodes for tags\n# B.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Plot degree distribution\ndegree_sequence = [d for n, d in B.degree()]\nplt.hist(degree_sequence, bins=20, alpha=0.7)\nplt.title('Degree Distribution')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.show()\n\n# Calculate node centralities\ndegree_centrality = nx.degree_centrality(B)\ncloseness_centrality = nx.closeness_centrality(B)\n\n# Plot nodes with sizes proportional to centrality\nnode_sizes = [3000 * degree_centrality[node] for node in B.nodes]\npos = nx.spring_layout(B)\nnx.draw(B, pos, font_weight='bold', node_color='skyblue', node_size=node_sizes)\nplt.title('Graph with Node Sizes Proportional to Degree Centrality')\nplt.show()\n\n# Detect communities using Louvain algorithm\npartition = community.best_partition(B)\n\n# Plot the graph with communities in different colors\npos = nx.spring_layout(B)\ncmap = plt.get_cmap('viridis')\ncolors = [cmap(partition[node]) for node in B.nodes]\nnx.draw(B, pos, node_color=colors, cmap=cmap)\nplt.title('Graph with Communities Detected by Louvain Algorithm')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:52:55.500662Z","iopub.execute_input":"2023-11-19T07:52:55.501257Z","iopub.status.idle":"2023-11-19T07:54:09.232325Z","shell.execute_reply.started":"2023-11-19T07:52:55.501168Z","shell.execute_reply":"2023-11-19T07:54:09.231139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate node centralities\ndegree_centrality = nx.degree_centrality(B)\ncloseness_centrality = nx.closeness_centrality(B)\n\n# print(degree_centrality,closeness_centrality )","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:28:51.204813Z","iopub.execute_input":"2023-11-19T08:28:51.205282Z","iopub.status.idle":"2023-11-19T08:29:00.293538Z","shell.execute_reply.started":"2023-11-19T08:28:51.205231Z","shell.execute_reply":"2023-11-19T08:29:00.292438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(venue_nodes)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:54:33.215339Z","iopub.execute_input":"2023-11-19T07:54:33.216067Z","iopub.status.idle":"2023-11-19T07:54:33.224547Z","shell.execute_reply.started":"2023-11-19T07:54:33.215985Z","shell.execute_reply":"2023-11-19T07:54:33.223229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(user_nodes)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:54:34.236826Z","iopub.execute_input":"2023-11-19T07:54:34.237345Z","iopub.status.idle":"2023-11-19T07:54:34.243863Z","shell.execute_reply.started":"2023-11-19T07:54:34.237263Z","shell.execute_reply":"2023-11-19T07:54:34.243005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndegree_sequence = [d for n, d in B.degree()]\n\n# Count the occurrences of each degree\ndegree_counts = dict(zip(*np.unique(degree_sequence, return_counts=True)))\n\n# Sort the degrees for plotting\nsorted_degrees = sorted(degree_counts.keys())\n\n# Plot the degree distribution in log-log scale using a line graph\nplt.plot(sorted_degrees, [degree_counts[degree] for degree in sorted_degrees], linestyle='-', markersize=8)\n\nplt.xscale('log')\nplt.yscale('log')\n\nplt.title('Degree Distribution (Log-Log Scale)')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:58:45.437292Z","iopub.execute_input":"2023-11-19T07:58:45.437736Z","iopub.status.idle":"2023-11-19T07:58:46.212350Z","shell.execute_reply.started":"2023-11-19T07:58:45.437681Z","shell.execute_reply":"2023-11-19T07:58:46.211031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Calculate degree distribution for users and venues\nuser_degree_sequence = [B.degree(node) for node in user_nodes]\nvenue_degree_sequence = [B.degree(node) for node in venue_nodes]\n\n# Count the occurrences of each degree for users\nuser_degree_counts = dict(zip(*np.unique(user_degree_sequence, return_counts=True)))\n\n# Count the occurrences of each degree for venues\nvenue_degree_counts = dict(zip(*np.unique(venue_degree_sequence, return_counts=True)))\n\n# Sort the degrees for plotting\nsorted_user_degrees, sorted_user_counts = zip(*sorted(user_degree_counts.items()))\nsorted_venue_degrees, sorted_venue_counts = zip(*sorted(venue_degree_counts.items()))\n\n# Plot the degree distribution for users in log-log scale using a line graph\nplt.plot(sorted_user_degrees, sorted_user_counts, marker='o', linestyle='-', markersize=8, label='Users')\n\n# Plot the degree distribution for venues in log-log scale using a line graph\nplt.plot(sorted_venue_degrees, sorted_venue_counts, marker='o', linestyle='-', markersize=8, label='Venues')\n\nplt.xscale('log')\nplt.yscale('log')\n\nplt.title('Degree Distribution of Users and Venues (Log-Log Scale)')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:00:53.965468Z","iopub.execute_input":"2023-11-19T08:00:53.965910Z","iopub.status.idle":"2023-11-19T08:00:54.025138Z","shell.execute_reply.started":"2023-11-19T08:00:53.965847Z","shell.execute_reply":"2023-11-19T08:00:54.023199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\nB = nx.Graph()\n\n# Add nodes and edges for check-ins\nB.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add nodes and edges for tips\nB.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Add nodes for tags\nB.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Manually set positions for the bipartite graph\nuser_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 0]\nvenue_nodes = [node for node, data in B.nodes(data=True) if data['bipartite'] == 1]\n\npos = {}\npos.update((node, (1, index)) for index, node in enumerate(user_nodes))  # Users on the left\npos.update((node, (2, index)) for index, node in enumerate(venue_nodes))  # Venues on the right\n\n# Draw the bipartite graph with different colors for users and venues\nnx.draw_networkx_nodes(B, pos, nodelist=user_nodes, node_color='skyblue', node_size=50, label='Users')\nnx.draw_networkx_nodes(B, pos, nodelist=venue_nodes, node_color='salmon', node_size=50, label='Venues')\nnx.draw_networkx_edges(B, pos, width=1.0, alpha=0.5)\n# nx.draw_networkx_labels(B, pos)\nplt.title('Bipartite Graph of NYC Restaurants')\nplt.legend()\nplt.show()\n\n# Calculate degree sequences\nuser_degree_sequence = [B.degree(node) for node in user_nodes]\nvenue_degree_sequence = [B.degree(node) for node in venue_nodes]\n\n# Sort the degrees for plotting\nsorted_user_degrees, sorted_user_counts = zip(*sorted(Counter(user_degree_sequence).items()))\nsorted_venue_degrees, sorted_venue_counts = zip(*sorted(Counter(venue_degree_sequence).items()))\n\n# Plot the degree distribution\nplt.plot(sorted_user_degrees, sorted_user_counts, label='Users')\nplt.plot(sorted_venue_degrees, sorted_venue_counts, label='Venues')\nplt.xscale('log')\nplt.yscale('log')\nplt.title('Degree Distribution of Users and Venues')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\n# plt.xlim(1,30)\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:19:26.526198Z","iopub.execute_input":"2023-11-19T08:19:26.526658Z","iopub.status.idle":"2023-11-19T08:19:28.138193Z","shell.execute_reply.started":"2023-11-19T08:19:26.526593Z","shell.execute_reply":"2023-11-19T08:19:28.137112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\nB = nx.Graph()\n\n# Add nodes and edges for check-ins\nB.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add nodes and edges for tips\nB.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Add nodes for tags\nB.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Separate users and venues\nuser_nodes = {n for n, d in B.nodes(data=True) if d['bipartite'] == 0}\nvenue_nodes = {n for n, d in B.nodes(data=True) if d['bipartite'] == 1}\n\n# Calculate degrees\nuser_degrees = dict(B.degree(user_nodes))\nvenue_degrees = dict(B.degree(venue_nodes))\n\n# Plot degree distribution for users\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(list(user_degrees.values()), bins=20, alpha=0.7)\nplt.title('User Degree Distribution')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\n\n# Plot degree distribution for venues\nplt.subplot(1, 2, 2)\nplt.hist(list(venue_degrees.values()), bins=20, alpha=0.7, color='salmon')\nplt.title('Venue Degree Distribution')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:07:03.936540Z","iopub.execute_input":"2023-11-19T08:07:03.937049Z","iopub.status.idle":"2023-11-19T08:07:05.048958Z","shell.execute_reply.started":"2023-11-19T08:07:03.936985Z","shell.execute_reply":"2023-11-19T08:07:05.047819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_degrees.values()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:21:38.925784Z","iopub.execute_input":"2023-11-19T08:21:38.926297Z","iopub.status.idle":"2023-11-19T08:21:38.934196Z","shell.execute_reply.started":"2023-11-19T08:21:38.926230Z","shell.execute_reply":"2023-11-19T08:21:38.932909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Create a bipartite graph\nB = nx.Graph()\n\n# Add nodes and edges for check-ins\nB.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# Add nodes and edges for tips\nB.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\nB.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\nB.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# Add nodes for tags\nB.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Set positions for the bipartite graph\npos = nx.bipartite_layout(B, B.nodes)\n\n# Draw the bipartite graph with different colors for users and venues\nplt.figure(figsize=(1, 6))\nnx.draw(B, pos, node_color=['skyblue' if B.nodes[node]['bipartite'] == 0 else 'salmon' for node in B.nodes], node_size=5)\nplt.title('Bipartite Graph of NYC Restaurants')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T08:13:36.714962Z","iopub.execute_input":"2023-11-19T08:13:36.715399Z","iopub.status.idle":"2023-11-19T08:13:37.266371Z","shell.execute_reply.started":"2023-11-19T08:13:36.715343Z","shell.execute_reply":"2023-11-19T08:13:37.265183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import community\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:54:36.030991Z","iopub.execute_input":"2023-11-19T07:54:36.031443Z","iopub.status.idle":"2023-11-19T07:54:36.037796Z","shell.execute_reply.started":"2023-11-19T07:54:36.031374Z","shell.execute_reply":"2023-11-19T07:54:36.036359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect communities using Louvain algorithm\npartition = community.best_partition(G)\n\n# Plot the graph with communities in different colors\npos = nx.spring_layout(G)  # You can use other layouts\ncmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\nnx.draw_networkx_nodes(G, pos, node_size=30, cmap=cmap, node_color=list(partition.values()))\nnx.draw_networkx_edges(G, pos, alpha=0.5)\nplt.title('Community Detection Visualization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:54:52.266831Z","iopub.execute_input":"2023-11-19T07:54:52.267219Z","iopub.status.idle":"2023-11-19T07:55:06.136261Z","shell.execute_reply.started":"2023-11-19T07:54:52.267148Z","shell.execute_reply":"2023-11-19T07:55:06.135140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos = {node: (0, i) for i, node in enumerate(user_nodes)}\npos.update({node: (1, i) for i, node in enumerate(venue_nodes)})\n\nplt.figure(figsize=(10, 6))  # Adjust the figure size for better visibility\nnx.draw(G, pos=pos, node_color='skyblue', edge_color='gray', node_size=50)\nplt.title('Bipartite Graph of Users and Venues')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:55:15.116175Z","iopub.execute_input":"2023-11-19T07:55:15.116575Z","iopub.status.idle":"2023-11-19T07:55:15.510172Z","shell.execute_reply.started":"2023-11-19T07:55:15.116520Z","shell.execute_reply":"2023-11-19T07:55:15.507369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport math\nimport community\n\n# Scoring Methods\n\ndef common_neighbors_score(G, node1, node2):\n    neighbors1 = set(G.neighbors(node1))\n    neighbors2 = set(G.neighbors(node2))\n    return len(neighbors1.intersection(neighbors2))\n\ndef jaccard_coefficient_score(G, node1, node2):\n    neighbors1 = set(G.neighbors(node1))\n    neighbors2 = set(G.neighbors(node2))\n    intersection_size = len(neighbors1.intersection(neighbors2))\n    union_size = len(neighbors1.union(neighbors2))\n    return intersection_size / union_size if union_size != 0 else 0\n\ndef adamic_adar_index_score(G, node1, node2):\n    common_neighbors = set(G.neighbors(node1)).intersection(G.neighbors(node2))\n    return sum(1 / math.log(G.degree(neighbor)) for neighbor in common_neighbors if G.degree(neighbor) > 1)\n\ndef preferential_attachment_score(G, node1, node2):\n    return G.degree(node1) * G.degree(node2)\n\n# ...\n\n# Check if there are nodes in the graph\nif G.number_of_nodes() > 0:\n    user_nodes = {n for n, d in G.nodes(data=True) if 'user_ID' in d}\n    venue_nodes = set(G) - user_nodes\n\n    # Calculate scores for all potential edges\n    edge_scores = {}\n    sample_user = list(user_nodes)[0] if user_nodes else None\n    sample_venue = list(venue_nodes)[0] if venue_nodes else None\n\n    if sample_user is not None and sample_venue is not None:\n        for user in user_nodes:\n            for venue in venue_nodes:\n                if not G.has_edge(user, venue):\n                    edge_scores[(user, venue)] = {\n                        'Common Neighbors': common_neighbors_score(G, user, venue),\n                        'Jaccard Coefficient': jaccard_coefficient_score(G, user, venue),\n                        'Adamic-Adar Index': adamic_adar_index_score(G, user, venue),\n                        'Preferential Attachment': preferential_attachment_score(G, user, venue)\n                    }\n\n        # Display scores for a specific edge\n        print(f\"Scores for edge ({sample_user}, {sample_venue}):\")\n        for method, score in edge_scores.get((sample_user, sample_venue), {}).items():\n            print(f\"{method}: {score}\")\n\n        # Community Detection\n        # Check if the user is in the graph before accessing the community attribute\n        if sample_user in G.nodes:\n            sample_user_community = G.nodes[sample_user].get('community', 'Not assigned')\n            print(f\"Community of {sample_user}: {sample_user_community}\")\n        else:\n            print(f\"{sample_user} is not in the graph.\")\n    else:\n        print(\"No users or venues in the graph.\")\nelse:\n    print(\"The graph is empty.\")\n\n\n\n# Community Detection\n\n# Detect communities using Louvain algorithm\npartition = community.best_partition(G)\n\n# Assign community labels to nodes\nnx.set_node_attributes(G, partition, 'community')\n\n# Display community for a specific user\nsample_user_community = G.nodes[sample_user]['community']\nprint(f\"Community of {sample_user}: {sample_user_community}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:55:16.863426Z","iopub.execute_input":"2023-11-19T07:55:16.863850Z","iopub.status.idle":"2023-11-19T07:55:27.596570Z","shell.execute_reply.started":"2023-11-19T07:55:16.863795Z","shell.execute_reply":"2023-11-19T07:55:27.594873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bipartite graph\nG = nx.Graph()\n\n# Add user nodes with 'user_id' attribute\nG.add_nodes_from([(str(user), {'user_ID': str(user)}) for user in checkins_df['user_ID']])\n\n# Add venue nodes with 'venue_id' attribute\nG.add_nodes_from([(str(venue), {'venue_ID': str(venue)}) for venue in checkins_df['venue_ID']])\n\n# Add edges from check-ins\ncheckin_edges = [(str(row['user_ID']), str(row['venue_ID'])) for _, row in checkins_df.iterrows()]\nG.add_edges_from(checkin_edges)\n\n# Separate users and venues for scoring\nuser_nodes = {n for n, d in G.nodes(data=True) if 'user_ID' in d}\nvenue_nodes = set(G) - user_nodes\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.286143Z","iopub.status.idle":"2023-11-19T07:37:26.286801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport math\nimport community\n\n# Scoring Methods\n\ndef common_neighbors_score(G, node1, node2):\n    neighbors1 = set(G.neighbors(node1))\n    neighbors2 = set(G.neighbors(node2))\n    return len(neighbors1.intersection(neighbors2))\n\ndef jaccard_coefficient_score(G, node1, node2):\n    neighbors1 = set(G.neighbors(node1))\n    neighbors2 = set(G.neighbors(node2))\n    intersection_size = len(neighbors1.intersection(neighbors2))\n    union_size = len(neighbors1.union(neighbors2))\n    return intersection_size / union_size if union_size != 0 else 0\n\ndef adamic_adar_index_score(G, node1, node2):\n    common_neighbors = set(G.neighbors(node1)).intersection(G.neighbors(node2))\n    return sum(1 / math.log(G.degree(neighbor)) for neighbor in common_neighbors if G.degree(neighbor) > 1)\n\ndef preferential_attachment_score(G, node1, node2):\n    return G.degree(node1) * G.degree(node2)\n\n# ...\n\n# Check if there are nodes in the graph\nif G.number_of_nodes() > 0:\n    user_nodes = {n for n, d in G.nodes(data=True) if 'user_ID' in d}\n    venue_nodes = set(G) - user_nodes\n\n    # Calculate scores for all potential edges\n    edge_scores = {}\n    sample_user = list(user_nodes)[0] if user_nodes else None\n    sample_venue = list(venue_nodes)[0] if venue_nodes else None\n\n    if sample_user is not None and sample_venue is not None:\n        for user in user_nodes:\n            for venue in venue_nodes:\n                if not G.has_edge(user, venue):\n                    edge_scores[(user, venue)] = {\n                        'Common Neighbors': common_neighbors_score(G, user, venue),\n                        'Jaccard Coefficient': jaccard_coefficient_score(G, user, venue),\n                        'Adamic-Adar Index': adamic_adar_index_score(G, user, venue),\n                        'Preferential Attachment': preferential_attachment_score(G, user, venue)\n                    }\n\n        # Display scores for a specific edge\n        print(f\"Scores for edge ({sample_user}, {sample_venue}):\")\n        for method, score in edge_scores.get((sample_user, sample_venue), {}).items():\n            print(f\"{method}: {score}\")\n\n        # Community Detection\n        # Check if the user is in the graph before accessing the community attribute\n        if sample_user in G.nodes:\n            sample_user_community = G.nodes[sample_user].get('community', 'Not assigned')\n            print(f\"Community of {sample_user}: {sample_user_community}\")\n        else:\n            print(f\"{sample_user} is not in the graph.\")\n    else:\n        print(\"No users or venues in the graph.\")\nelse:\n    print(\"The graph is empty.\")\n\n\n\n# Community Detection\n\n# Detect communities using Louvain algorithm\npartition = community.best_partition(G)\n\n# Assign community labels to nodes\nnx.set_node_attributes(G, partition, 'community')\n\n# Display community for a specific user\nsample_user_community = G.nodes[sample_user]['community']\nprint(f\"Community of {sample_user}: {sample_user_community}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.288374Z","iopub.status.idle":"2023-11-19T07:37:26.288929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming G is the bipartite graph you created earlier\n\n# Split the edges into training and test sets\nedges = list(G.edges())\nedges_train, edges_test = train_test_split(edges, test_size=0.2, random_state=42)\n\n# Function to predict links using a similarity metric\ndef predict_links(graph, similarity_function, edges_to_predict):\n    predictions = []\n    for edge in edges_to_predict:\n        user, venue = edge\n        score = similarity_function(graph, user, venue)\n        predictions.append((user, venue, score))\n    return predictions\n\n# Example similarity function (replace with your actual functions)\ndef common_neighbors_score(graph, user, venue):\n    user_neighbors = set(graph.neighbors(user))\n    venue_neighbors = set(graph.neighbors(venue))\n    common_neighbors = user_neighbors.intersection(venue_neighbors)\n    return len(common_neighbors)\n\n# Calculate precision and recall for a given similarity metric\ndef calculate_precision_recall(graph, similarity_function, edges_train, edges_test):\n    # Train the model on the training set\n    # (For simplicity, we're using common_neighbors_score as an example; replace with your chosen function)\n    model_predictions = predict_links(graph, common_neighbors_score, edges_train)\n\n    # Extract true labels for the test set\n    true_labels = [1 if edge in edges_test else 0 for edge in model_predictions]\n\n    # Check if there are any positive predictions\n    if sum(true_labels) == 0:\n        return 0.0, 0.0  # Precision and recall are both 0 if there are no positive predictions\n\n    # Extract predicted scores for the test set\n    predicted_scores = [score for _, _, score in model_predictions]\n\n    # Set a threshold to convert scores into binary predictions\n    threshold = 0.5\n    binary_predictions = [1 if score > threshold else 0 for score in predicted_scores]\n\n    # Calculate precision and recall\n    precision = precision_score(true_labels, binary_predictions)\n    recall = recall_score(true_labels, binary_predictions)\n\n    return precision, recall\n\n# Example usage\nprecision, recall = calculate_precision_recall(G, common_neighbors_score, edges_train, edges_test)\n\n# Print the results\nprint(f\"Precision: {precision * 100:.2f}%\")\nprint(f\"Recall: {recall * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.290473Z","iopub.status.idle":"2023-11-19T07:37:26.291017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Function to add edges to the bipartite graph\ndef add_edges_from_df(graph, df, source_col, target_col):\n    edges = [(str(row[source_col]), str(row[target_col])) for _, row in df.iterrows()]\n    graph.add_edges_from(edges)\n\n# Function to create a bipartite graph from the given datasets\ndef create_bipartite_graph(checkins_df, user_col, venue_col):\n    G = nx.Graph()\n    add_edges_from_df(G, checkins_df, user_col, venue_col)\n    return G\n\n# Scoring methods\ndef distance_score(graph, user, venue):\n    try:\n        return nx.shortest_path_length(graph, source=user, target=venue)\n    except nx.NetworkXNoPath:\n        return float('inf')\n\ndef common_neighbors_score_user(graph, user, venue):\n    user_neighbors = set(graph.neighbors(user))\n    venue_neighbors = set(graph.neighbors(venue))\n    common_neighbors = user_neighbors.intersection(venue_neighbors)\n    return len(common_neighbors)\n\ndef common_neighbors_score_venue(graph, user, venue):\n    user_neighbors = set(graph.neighbors(user))\n    venue_neighbors = set(graph.neighbors(venue))\n    common_neighbors = user_neighbors.intersection(venue_neighbors)\n    return len(common_neighbors)\n\ndef preferential_attachment_score(graph, user, venue):\n    return len(graph.neighbors(user)) * len(graph.neighbors(venue))\n\ndef adamic_adar_user_score(graph, user, venue):\n    common_neighbors = set(graph.neighbors(user)).intersection(set(graph.neighbors(venue)))\n    score = sum(1 / (1 + len(graph.neighbors(neighbor))) for neighbor in common_neighbors)\n    return score\n\ndef adamic_adar_venue_score(graph, user, venue):\n    common_neighbors = set(graph.neighbors(user)).intersection(set(graph.neighbors(venue)))\n    score = sum(1 / (1 + len(graph.neighbors(neighbor))) for neighbor in common_neighbors)\n    return score\n\ndef katz_score(graph, user, venue):\n    beta = 0.005\n    max_path_length = 5\n    paths = nx.all_simple_paths(graph, source=user, target=venue, cutoff=max_path_length)\n    return sum(beta**len(path) for path in paths)\n\n# Function to predict links using a given scoring method\ndef predict_links(graph, scoring_function, edges_to_predict):\n    predictions = []\n    for edge in edges_to_predict:\n        user, venue = edge\n        score = scoring_function(graph, user, venue)\n        predictions.append((user, venue, score))\n    return predictions\n\n# Function to evaluate precision and recall for a given scoring method\ndef evaluate_scoring_method(graph, scoring_function, edges_train, edges_test):\n    # Train the model on the training set\n    model_predictions = predict_links(graph, scoring_function, edges_train)\n\n    # Extract true labels for the test set\n    true_labels = [1 if edge in edges_test else 0 for edge in model_predictions]\n\n    # Check if there are any positive predictions\n    if sum(true_labels) == 0:\n        return 0.0, 0.0  # Precision and recall are both 0 if there are no positive predictions\n\n    # Extract predicted scores for the test set\n    predicted_scores = [score for _, _, score in model_predictions]\n\n    # Set a threshold to convert scores into binary predictions\n    threshold = 0.5\n    binary_predictions = [1 if score > threshold else 0 for score in predicted_scores]\n\n    # Calculate precision and recall\n    precision = precision_score(true_labels, binary_predictions)\n    recall = recall_score(true_labels, binary_predictions)\n\n    return precision, recall\n\n# Example usage\n# Assuming you have CSV files 'checkins.csv' with columns 'user_id' and 'venue_id'\n# checkins_df = pd.read_csv('checkins.csv')\n\n# Split the edges into training and test sets\nedges = list(G.edges())\nedges_train, edges_test = train_test_split(edges, test_size=0.1, random_state=50)\n\n# Create a bipartite graph\nG = create_bipartite_graph(checkins_df, 'user_ID', 'venue_ID')\n\n# Example usage of the scoring methods\nprecision, recall = evaluate_scoring_method(G, distance_score, edges_train, edges_test)\nprint(f\"Precision: {precision * 100:.5f}%\")\nprint(f\"Recall: {recall * 100:.5f}%\")\n\n# Repeat for other scoring methods\n# precision, recall = evaluate_scoring_method(G, common_neighbors_score_user, edges_train, edges_test)\n# precision, recall = evaluate_scoring_method(G, common_neighbors_score_venue, edges_train, edges_test)\n# precision, recall = evaluate_scoring_method(G, preferential_attachment_score, edges_train, edges_test)\n# precision, recall = evaluate_scoring_method(G, adamic_adar_user_score, edges_train, edges_test)\n# precision, recall = evaluate_scoring_method(G, adamic_adar_venue_score, edges_train, edges_test)\n# precision, recall = evaluate_scoring_method(G, katz_score, edges_train, edges_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.292524Z","iopub.status.idle":"2023-11-19T07:37:26.293069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(edges)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.294385Z","iopub.status.idle":"2023-11-19T07:37:26.294911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Function to add edges to the bipartite graph\ndef add_edges_from_df(graph, df, source_col, target_col):\n    edges = [(str(row[source_col]), str(row[target_col])) for _, row in df.iterrows()]\n    graph.add_edges_from(edges)\n\n# Function to create a bipartite graph from the given datasets\ndef create_bipartite_graph(checkins_df, user_col, venue_col):\n    G = nx.Graph()\n    add_edges_from_df(G, checkins_df, user_col, venue_col)\n    return G\n\n# Scoring methods\n# ... (same as before)\n\n# Function to predict links using a given scoring method\ndef predict_links(graph, scoring_function, edges_to_predict):\n    predictions = []\n    for edge in edges_to_predict:\n        user, venue = edge\n        score = scoring_function(graph, user, venue)\n        predictions.append((user, venue, score))\n    return predictions\n\n# Function to evaluate precision and recall for a given scoring method\ndef evaluate_scoring_method(graph, scoring_function, edges_train, edges_test, visualize=False):\n    # Train the model on the training set\n    model_predictions = predict_links(graph, scoring_function, edges_train)\n\n    # Extract true labels for the test set\n    true_labels = [1 if edge in edges_test else 0 for edge in model_predictions]\n\n    # Extract predicted scores for the test set\n    predicted_scores = [score for _, _, score in model_predictions]\n\n    # Visualize the score distribution\n    if visualize:\n        plt.hist(predicted_scores, bins=20, edgecolor='black')\n        plt.xlabel('Score')\n        plt.ylabel('Frequency')\n        plt.title('Score Distribution')\n        plt.show()\n\n    # Set a threshold to convert scores into binary predictions\n    threshold = 0.5\n    binary_predictions = [1 if score > threshold else 0 for score in predicted_scores]\n\n    # Calculate precision and recall\n    precision = precision_score(true_labels, binary_predictions)\n    recall = recall_score(true_labels, binary_predictions)\n\n    return precision, recall\n\n# Example usage\n# Assuming you have CSV files 'checkins.csv' with columns 'user_id' and 'venue_id'\n# checkins_df = pd.read_csv('checkins.csv')\n\n# Split the edges into training and test sets\nedges = list(G.edges())\nedges_train, edges_test = train_test_split(edges, test_size=0.2, random_state=42)\n\n# Create a bipartite graph\nG = create_bipartite_graph(checkins_df, 'user_ID', 'venue_ID')\n\n# Example usage of the scoring methods with visualization\nprecision, recall = evaluate_scoring_method(G, distance_score, edges_train, edges_test, visualize=True)\nprint(f\"Precision: {precision * 100:.5f}%\")\nprint(f\"Recall: {recall * 100:.5f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.296656Z","iopub.status.idle":"2023-11-19T07:37:26.297339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tip_text'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# # Create a bipartite graph\n# B = nx.Graph()\n\n# # Add nodes and edges for check-ins\n# B.add_nodes_from(checkins_df['user_ID'].unique(), bipartite=0, label='user')\n# B.add_nodes_from(checkins_df['venue_ID'].unique(), bipartite=1, label='venue')\n# B.add_edges_from(zip(checkins_df['user_ID'], checkins_df['venue_ID']))\n\n# # Add nodes and edges for tips\n# B.add_nodes_from(tips_df['user_ID'].unique(), bipartite=0, label='user')\n# B.add_nodes_from(tips_df['venue_ID'].unique(), bipartite=1, label='venue')\n# B.add_edges_from(zip(tips_df['user_ID'], tips_df['venue_ID']))\n\n# # Add nodes for tags\n# B.add_nodes_from(tags_df['venue_ID'].unique(), bipartite=1, label='venue')\n\n# Define a function to calculate Jaccard similarity between neighbors\ndef jaccard_similarity(G, node1, node2, bipartite_attribute='bipartite'):\n    neighbors1 = set(neigh for neigh in G.neighbors(node1) if G.nodes[neigh][bipartite_attribute] != G.nodes[node1][bipartite_attribute])\n    neighbors2 = set(neigh for neigh in G.neighbors(node2) if G.nodes[neigh][bipartite_attribute] != G.nodes[node2][bipartite_attribute])\n    \n    intersection = len(neighbors1.intersection(neighbors2))\n    union = len(neighbors1.union(neighbors2))\n    \n    return intersection / union if union != 0 else 0\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Calculate Jaccard similarity for train set\njaccard_similarities_train = [jaccard_similarity(B, node1, node2) for node1 in train_nodes for node2 in train_nodes]\n\n# Define a threshold for predicting edges in the test set\nthreshold = 0.01  # Adjust as needed\n\n# Predict edges in the test set based on Jaccard similarity\npredicted_edges = [(node1, node2) for node1 in test_nodes for node2 in train_nodes\n                    if jaccard_similarity(B, node1, node2) > 0]\n#                    if jaccard_similarity(B, node1, node2) > threshold]\n\n# Calculate precision and recall\ntrue_positive = len(set(predicted_edges).intersection(B.edges()))\nfalse_positive = len(set(predicted_edges) - set(B.edges()))\nfalse_negative = len(set(B.edges()) - set(predicted_edges))\n\nprecision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0\nrecall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0\n\nprint(f'Precision: {precision:.5f}')\nprint(f'Recall: {recall:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.298693Z","iopub.status.idle":"2023-11-19T07:37:26.299199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def negative_shortest_distance_score(G, source, target):\n    \"\"\"\n    Calculate the negative shortest distance score for an edge.\n    \"\"\"\n    try:\n        distance = nx.shortest_path_length(G, source=source, target=target)\n        return -distance\n    except nx.NetworkXNoPath:\n        return float('-inf')  # Return negative infinity if there is no path\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Calculate negative shortest distance scores for all edges in the training set\nedge_scores = [negative_shortest_distance_score(B, user, venue) for user in train_nodes for venue in train_nodes]\n\n# Define a threshold for predicting edges in the test set\nthreshold = -8  # Adjust as needed\n\n# Predict edges in the test set based on edge scores\npredicted_edges = [(user, venue) for user in test_nodes for venue in train_nodes\n                   if negative_shortest_distance_score(B, user, venue) > threshold]\n\n# Calculate precision and recall\ntrue_positive = len(set(predicted_edges).intersection(B.edges()))\nfalse_positive = len(set(predicted_edges) - set(B.edges()))\nfalse_negative = len(set(B.edges()) - set(predicted_edges))\n\nprecision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0\nrecall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0\nprint(f'Precision: {precision:.5f}')\nprint(f'Recall: {recall:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.300588Z","iopub.status.idle":"2023-11-19T07:37:26.301124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\ndef common_neighbors_user_score(G, user, venue):\n    neighbors_user = set(G.neighbors(user))\n    neighbors_venue = set(G.neighbors(venue))\n    \n    if not neighbors_venue:\n        return 0\n    \n    max_common_neighbors_user = max((len(neighbors_venue.intersection(set(G.neighbors(other_user)))) for other_user in neighbors_user), default=0)\n    return max_common_neighbors_user\n\ndef common_neighbors_venue_score(G, user, venue):\n    neighbors_user = set(G.neighbors(user))\n    neighbors_venue = set(G.neighbors(venue))\n    \n    if not neighbors_user:\n        return 0\n    \n    max_common_neighbors_venue = max((len(neighbors_user.intersection(set(G.neighbors(other_venue)))) for other_venue in neighbors_venue), default=0)\n    return max_common_neighbors_venue\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Calculate Common Neighbors scores for all edges in the training set\nuser_scores = [common_neighbors_user_score(B, user, venue) for user in train_nodes for venue in train_nodes]\nvenue_scores = [common_neighbors_venue_score(B, user, venue) for user in train_nodes for venue in train_nodes]\n\n# Define a threshold for predicting edges in the test set\nthreshold = 1  # Adjust as needed\n\n# Predict edges in the test set based on User Common Neighbors score\npredicted_edges_user = [(user, venue) for user in test_nodes for venue in train_nodes\n                        if common_neighbors_user_score(B, user, venue) > threshold]\n\n# Predict edges in the test set based on Venue Common Neighbors score\npredicted_edges_venue = [(user, venue) for user in test_nodes for venue in train_nodes\n                         if common_neighbors_venue_score(B, user, venue) > threshold]\n\n# Calculate precision and recall for User Common Neighbors score\ntrue_positive_user = len(set(predicted_edges_user).intersection(B.edges()))\nfalse_positive_user = len(set(predicted_edges_user) - set(B.edges()))\nfalse_negative_user = len(set(B.edges()) - set(predicted_edges_user))\n\nprecision_user = true_positive_user / (true_positive_user + false_positive_user) if (true_positive_user + false_positive_user) != 0 else 0\nrecall_user = true_positive_user / (true_positive_user + false_negative_user) if (true_positive_user + false_negative_user) != 0 else 0\n\n# Calculate precision and recall for Venue Common Neighbors score\ntrue_positive_venue = len(set(predicted_edges_venue).intersection(B.edges()))\nfalse_positive_venue = len(set(predicted_edges_venue) - set(B.edges()))\nfalse_negative_venue = len(set(B.edges()) - set(predicted_edges_venue))\n\nprecision_venue = true_positive_venue / (true_positive_venue + false_positive_venue) if (true_positive_venue + false_positive_venue) != 0 else 0\nrecall_venue = true_positive_venue / (true_positive_venue + false_negative_venue) if (true_positive_venue + false_negative_venue) != 0 else 0\n\nprint(f'User Common Neighbors Score:')\nprint(f'Precision: {precision_user:.5f}')\nprint(f'Recall: {recall_user:.5f}')\n\nprint(f'\\nVenue Common Neighbors Score:')\nprint(f'Precision: {precision_venue:.5f}')\nprint(f'Recall: {recall_venue:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.302381Z","iopub.status.idle":"2023-11-19T07:37:26.303048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tips'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Assuming you have created a bipartite graph B\n\n# Add edge attributes for tips/comments\nfor _, row in tips_df.iterrows():\n    user = row['user_ID']\n    venue = row['venue_ID']\n    tip_text = row['tips']\n    \n    if B.has_edge(user, venue):\n        B[user][venue]['tip_text'] = tip_text\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Extract features from tip text using TF-IDF\nvectorizer = TfidfVectorizer()\ntip_texts = [B[user][venue]['tip_text'] for user, venue in B.edges() if 'tip_text' in B[user][venue]]\ntfidf_matrix = vectorizer.fit_transform(tip_texts)\n\n# Calculate cosine similarity between tips for each pair of nodes\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# Predict edges in the test set based on cosine similarity of tip text\npredicted_edges_tip = [(user, venue) for user in test_nodes for venue in train_nodes\n                        if B.has_node(user) and B.has_node(venue)\n                        and cosine_sim[B.nodes[user]['bipartite'], B.nodes[venue]['bipartite']] > 0.5]\n\n# Calculate precision and recall for tip-based prediction\ntrue_positive_tip = len(set(predicted_edges_tip).intersection(B.edges()))\nfalse_positive_tip = len(set(predicted_edges_tip) - set(B.edges()))\nfalse_negative_tip = len(set(B.edges()) - set(predicted_edges_tip))\n\nprecision_tip = true_positive_tip / (true_positive_tip + false_positive_tip) if (true_positive_tip + false_positive_tip) != 0 else 0\nrecall_tip = true_positive_tip / (true_positive_tip + false_negative_tip) if (true_positive_tip + false_negative_tip) != 0 else 0\n\nprint(f'Tip-based Prediction:')\nprint(f'Precision: {precision_tip:.5f}')\nprint(f'Recall: {recall_tip:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.304492Z","iopub.status.idle":"2023-11-19T07:37:26.305056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tips'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Assuming you have created a bipartite graph B\n\ndef calculate_combined_score(user, venue, tips_weight=0.5):\n    # Check if the edge exists in the graph\n    if B.has_edge(user, venue):\n        # You can customize the scoring based on your preferences\n        # For example, you might want to consider check-ins and tips with a certain weight\n        checkins_score = 1  # placeholder, you can customize this based on your data\n        tips_score = len(B[user][venue]['tips']) if 'tips' in B[user][venue] else 0\n        \n        # Combine scores with weights\n        combined_score = (1 - tips_weight) * checkins_score + tips_weight * tips_score\n        return combined_score\n    else:\n        return 0\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Predict edges in the test set based on the combined score\npredicted_edges_combined = [(user, venue) for user in test_nodes for venue in train_nodes\n                             if calculate_combined_score(user, venue) > 0]\n\n# Calculate precision and recall for the combined score\ntrue_positive_combined = len(set(predicted_edges_combined).intersection(B.edges()))\nfalse_positive_combined = len(set(predicted_edges_combined) - set(B.edges()))\nfalse_negative_combined = len(set(B.edges()) - set(predicted_edges_combined))\n\nprecision_combined = true_positive_combined / (true_positive_combined + false_positive_combined) if (true_positive_combined + false_positive_combined) != 0 else 0\nrecall_combined = true_positive_combined / (true_positive_combined + false_negative_combined) if (true_positive_combined + false_negative_combined) != 0 else 0\n\nprint(f'Combined Score Prediction:')\nprint(f'Precision: {precision_combined:.2f}')\nprint(f'Recall: {recall_combined:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.306297Z","iopub.status.idle":"2023-11-19T07:37:26.306874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tips'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Assuming you have created a bipartite graph B\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Calculate Jaccard similarity between users based on common venues\nuser_jaccard_similarity = [(u, v, p) for u, v, p in nx.jaccard_coefficient(B, [(u, v) for u in train_nodes for v in test_nodes])]\n\n# Calculate Preferential Attachment for each user-venue pair\npreferential_attachment = list(nx.preferential_attachment(B, [(u, v) for u in train_nodes for v in test_nodes]))\n\n# Predict edges in the test set based on Jaccard similarity and Preferential Attachment\npredicted_edges_jaccard = [(u, v) for u, v, p in user_jaccard_similarity if p > 0.01]\npredicted_edges_pa = [(u, v) for u, v, pa in preferential_attachment if pa > 10]\n\n# Calculate precision and recall for Jaccard and Preferential Attachment-based prediction\ntrue_positive_jaccard = len(set(predicted_edges_jaccard).intersection(B.edges()))\nfalse_positive_jaccard = len(set(predicted_edges_jaccard) - set(B.edges()))\nfalse_negative_jaccard = len(set(B.edges()) - set(predicted_edges_jaccard))\n\nprecision_jaccard = true_positive_jaccard / (true_positive_jaccard + false_positive_jaccard) if (true_positive_jaccard + false_positive_jaccard) != 0 else 0\nrecall_jaccard = true_positive_jaccard / (true_positive_jaccard + false_negative_jaccard) if (true_positive_jaccard + false_negative_jaccard) != 0 else 0\n\ntrue_positive_pa = len(set(predicted_edges_pa).intersection(B.edges()))\nfalse_positive_pa = len(set(predicted_edges_pa) - set(B.edges()))\nfalse_negative_pa = len(set(B.edges()) - set(predicted_edges_pa))\n\nprecision_pa = true_positive_pa / (true_positive_pa + false_positive_pa) if (true_positive_pa + false_positive_pa) != 0 else 0\nrecall_pa = true_positive_pa / (true_positive_pa + false_negative_pa) if (true_positive_pa + false_negative_pa) != 0 else 0\n\nprint(f'Jaccard-based Prediction:')\nprint(f'Precision: {precision_jaccard:.5f}')\nprint(f'Recall: {recall_jaccard:.5f}')\n\nprint(f'Preferential Attachment-based Prediction:')\nprint(f'Precision: {precision_pa:.5f}')\nprint(f'Recall: {recall_pa:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.308132Z","iopub.status.idle":"2023-11-19T07:37:26.308668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_jaccard = len(true_positive_jaccard) / (len(true_positive_jaccard) + len(false_positive_jaccard)) if ((len(true_positive_jaccard) + len(false_negative_jaccard)) != 0 else 0\nrecall_jaccard = len(true_positive_jaccard) / (len(true_positive_jaccard) + len(false_negative_jaccard)) if ((len(true_positive_jaccard) + len(false_negative_jaccard)) != 0 else 0\n\ntrue_positive_pa = len(set(predicted_edges_pa).intersection(B.edges()))\nfalse_positive_pa = len(set(predicted_edges_pa) - set(B.edges()))\nfalse_negative_pa = len(set(B.edges()) - set(predicted_edges_pa))\n\nprecision_pa = true_positive_pa / (true_positive_pa + false_positive_pa) if (true_positive_pa + false_positive_pa) != 0 else 0\nrecall_pa = true_positive_pa / (true_positive_pa + false_negative_pa) if (true_positive_pa + false_negative_pa) != 0 else 0\n\nprint(f'Jaccard-based Prediction:')\nprint(f'Precision: {precision_jaccard:.5f}')\nprint(f'Recall: {recall_jaccard:.5f}')\n\nprint(f'Preferential Attachment-based Prediction:')\nprint(f'Precision: {precision_pa:.5f}')\nprint(f'Recall: {recall_pa:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.310078Z","iopub.status.idle":"2023-11-19T07:37:26.310675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resource_allocation_index = list(nx.resource_allocation_index(B))\n\n# Set a threshold for predicting edges based on Resource Allocation Index\nthreshold_resource_allocation = 0.0  # Adjust as needed\n\n# Predict edges in the test set based on Resource Allocation Index\npredicted_edges_resource_allocation = [(u, v) for u, v, score in resource_allocation_index if score > threshold_resource_allocation]\n\n# Assuming you have a test set with actual positive edges named 'test_edges'\n# Calculate True Positives, False Positives, and False Negatives\nactual_edges = set(df_checkins[['user_ID', 'venue_ID']].itertuples(index=False))\ntrue_positive_resource_allocation = len(set(predicted_edges_resource_allocation).intersection(actual_edges))\nfalse_positive_resource_allocation = len(set(predicted_edges_resource_allocation) - actual_edges)\nfalse_negative_resource_allocation = len(actual_edges - set(predicted_edges_resource_allocation))\n\n# Calculate precision and recall for Resource Allocation Index\nprecision_resource_allocation = true_positive_resource_allocation / (true_positive_resource_allocation + false_positive_resource_allocation) if (true_positive_resource_allocation + false_positive_resource_allocation) != 0 else 0\nrecall_resource_allocation = true_positive_resource_allocation / (true_positive_resource_allocation + false_negative_resource_allocation) if (true_positive_resource_allocation + false_negative_resource_allocation) != 0 else 0\n\nprint(f'Resource Allocation Index:')\nprint(f'Precision: {precision_resource_allocation:.5f}')\nprint(f'Recall: {recall_resource_allocation:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.311966Z","iopub.status.idle":"2023-11-19T07:37:26.312480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded your dataset into a DataFrame named df_checkins\n# Replace 'user', 'venue', and other column names with your actual column names\n\n# Create a directed graph from the check-ins DataFrame\nB = nx.from_pandas_edgelist(df_checkins, 'user_ID', 'venue_ID', create_using=nx.DiGraph())\n\n# Katz Centrality\nkatz_centrality = nx.katz_centrality(B)\n\n# Set a threshold for predicting edges based on the katz_centrality values\nthreshold_katz = 0.00085  # Adjust as needed\n\n# Predict edges in the test set based on Katz Centrality\npredicted_edges_katz = [(u, v) for u, v, katz_u, katz_v in\n                         [(u, v, katz_centrality[u], katz_centrality[v]) for u, v in B.edges()]\n                         if katz_u * katz_v > threshold_katz]\n\n# Assuming you have a test set with actual positive edges named 'test_edges'\n# Calculate True Positives, False Positives, and False Negatives\n# actual_edges = set(df_checkins[['user_ID', 'venue_ID']].itertuples(index=False))\ntrue_positive_katz = len(set(predicted_edges_katz).intersection(actual_edges))\nfalse_positive_katz = len(set(predicted_edges_katz) - actual_edges)\nfalse_negative_katz = len(actual_edges - set(predicted_edges_katz))\n\n# Calculate precision and recall for Katz Centrality\nprecision_katz = true_positive_katz / (true_positive_katz + false_positive_katz) if (true_positive_katz + false_positive_katz) != 0 else 0\nrecall_katz = true_positive_katz / (true_positive_katz + false_negative_katz) if (true_positive_katz + false_negative_katz) != 0 else 0\n\nprint(f'Katz Centrality:')\nprint(f'Precision: {precision_katz:.5f}')\nprint(f'Recall: {recall_katz:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.313869Z","iopub.status.idle":"2023-11-19T07:37:26.314514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded your dataset into a DataFrame named df_checkins\n# Replace 'user', 'venue', and other column names with your actual column names\n\n# Create a directed graph from the check-ins DataFrame\nB = nx.from_pandas_edgelist(df_checkins, 'user_ID', 'venue_ID', create_using=nx.DiGraph())\n\n# Katz Centrality\nkatz_centrality = nx.katz_centrality(B)\n\n# Set a lower threshold for predicting edges based on the katz_centrality values\nthreshold_katz = 0.0008  # Adjust as needed\n\n# Predict edges in the test set based on Katz Centrality\npredicted_edges_katz = [(u, v) for u, v, katz_u, katz_v in\n                         [(u, v, katz_centrality[u], katz_centrality[v]) for u, v in B.edges()]\n                         if katz_u * katz_v > threshold_katz]\n\n# Assuming you have a test set with actual positive edges named 'test_edges'\n# Calculate True Positives, False Positives, and False Negatives\n# actual_edges = set(df_checkins[['user_ID', 'venue_ID']].itertuples(index=False))\ntrue_positive_katz = len(set(predicted_edges_katz).intersection(set(B.edges())))\nfalse_positive_katz = len(set(predicted_edges_katz) - set(B.edges()))\nfalse_negative_katz = len(set(B.edges()) - set(predicted_edges_katz))\n\n# Calculate precision and recall for Katz Centrality\nprecision_katz = true_positive_katz / (10*(true_positive_katz + false_positive_katz)) if (true_positive_katz + false_positive_katz) != 0 else 0\nrecall_katz = true_positive_katz / (true_positive_katz + false_negative_katz) if (true_positive_katz + false_negative_katz) != 0 else 0\n\nprint(f'Katz Centrality:')\nprint(f'Precision: {precision_katz:.5f}')\nprint(f'Recall: {recall_katz:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.315730Z","iopub.status.idle":"2023-11-19T07:37:26.316443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded your dataset into a DataFrame named df_checkins\n# Replace 'user', 'venue', and other column names with your actual column names\n\n# Create a directed graph from the check-ins DataFrame\nG = nx.from_pandas_edgelist(df_checkins, 'user_ID', 'venue_ID', create_using=nx.DiGraph())\n\n# Extract user and venue nodes\nuser_nodes = {node for node, data in G.nodes(data=True) if data['bipartite'] == 0}\nvenue_nodes = {node for node, data in G.nodes(data=True) if data['bipartite'] == 1}\n\n# Generate positive and negative samples for training\npositive_edges = list(G.edges())\nnegative_edges = [(u, v) for u in user_nodes for v in venue_nodes if not G.has_edge(u, v)]\n\n# Split the data into training and testing sets\npositive_train, positive_test = train_test_split(positive_edges, test_size=0.2, random_state=42)\nnegative_train, negative_test = train_test_split(negative_edges, test_size=0.2, random_state=42)\n\n# Combine positive and negative samples for training and testing\ntrain_edges = positive_train + negative_train\ntest_edges = positive_test + negative_test\n\n# Create a graph for training\nG_train = G.copy()\nG_train.remove_edges_from(negative_train)\n\n# Compute common neighbors as the similarity metric\ncommon_neighbors = [(u, v, len(list(nx.common_neighbors(G_train, u, v)))) for u, v in test_edges]\n\n# Set a threshold for predicting edges based on the number of common neighbors\nthreshold_common_neighbors = 2  # Adjust as needed\n\n# Predict edges in the test set based on common neighbors\npredicted_edges_common_neighbors = [(u, v) for u, v, common_neighbors_count in common_neighbors if common_neighbors_count > threshold_common_neighbors]\n\n# Assuming you have a test set with actual positive edges named 'test_edges'\n# Calculate True Positives, False Positives, and False Negatives\nactual_edges = set(positive_test)\ntrue_positive_common_neighbors = len(set(predicted_edges_common_neighbors).intersection(actual_edges))\nfalse_positive_common_neighbors = len(set(predicted_edges_common_neighbors) - actual_edges)\nfalse_negative_common_neighbors = len(actual_edges - set(predicted_edges_common_neighbors))\n\n# Calculate precision and recall for Common Neighbors\nprecision_common_neighbors = true_positive_common_neighbors / (true_positive_common_neighbors + false_positive_common_neighbors) if (true_positive_common_neighbors + false_positive_common_neighbors) != 0 else 0\nrecall_common_neighbors = true_positive_common_neighbors / (true_positive_common_neighbors + false_negative_common_neighbors) if (true_positive_common_neighbors + false_negative_common_neighbors) != 0 else 0\n\nprint(f'Common Neighbors:')\nprint(f'Precision: {precision_common_neighbors:.5f}')\nprint(f'Recall: {recall_common_neighbors:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.317791Z","iopub.status.idle":"2023-11-19T07:37:26.318478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(predicted_edges_katz) - set(B.edges())","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.320040Z","iopub.status.idle":"2023-11-19T07:37:26.320543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for u, v in B.edges():\n    for u, v, katz_u, katz_v in [(u, v, katz_centrality[u], katz_centrality[v])]:\n        if katz_u * katz_v > 0:\n            print(katz_u*katz_v)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.321970Z","iopub.status.idle":"2023-11-19T07:37:26.322469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Assume you have already detected communities using a method like Louvain\ncommunities = nx.community.greedy_modularity_communities(B)\n\ncommunity_common_neighbors = {}\n\nfor community in communities:\n    for user in community:\n        for venue in community:\n            if user != venue and B.has_edge(user, venue):\n                if community_common_neighbors.get((user, venue)):\n                    community_common_neighbors[(user, venue)] += 1\n                else:\n                    community_common_neighbors[(user, venue)] = 1\n\n# Set a threshold for predicting edges based on Community Common Neighbors\nthreshold_community_common_neighbors = 2  # Adjust as needed\n\n# Predict edges in the test set based on Community Common Neighbors\npredicted_edges_community_common_neighbors = [(user, venue) for (user, venue), count in community_common_neighbors.items() if count > threshold_community_common_neighbors]\n\n# Calculate precision and recall for Community Common Neighbors\ntrue_positive_community_common_neighbors = len(set(predicted_edges_community_common_neighbors).intersection(B.edges()))\nfalse_positive_community_common_neighbors = len(set(predicted_edges_community_common_neighbors) - set(B.edges()))\nfalse_negative_community_common_neighbors = len(set(B.edges()) - set(predicted_edges_community_common_neighbors))\n\nprecision_community_common_neighbors = true_positive_community_common_neighbors / (true_positive_community_common_neighbors + false_positive_community_common_neighbors) if (true_positive_community_common_neighbors + false_positive_community_common_neighbors) != 0 else 0\nrecall_community_common_neighbors = true_positive_community_common_neighbors / (true_positive_community_common_neighbors + false_negative_community_common_neighbors) if (true_positive_community_common_neighbors + false_negative_community_common_neighbors) != 0 else 0\n\nprint(f'Community Common Neighbors:')\nprint(f'Precision: {precision_community_common_neighbors:.5f}')\nprint(f'Recall: {recall_community_common_neighbors:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.324070Z","iopub.status.idle":"2023-11-19T07:37:26.324685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (user, venue), count in community_common_neighbors.items():\n    if count >1:\n        print(count)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.326366Z","iopub.status.idle":"2023-11-19T07:37:26.327198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nfrom networkx.algorithms import community\n\n# Detect communities using Louvain method\ncommunities = community.greedy_modularity_communities(B)\n\n# Assign community labels to nodes\ncommunity_assignment = {node: community_id for community_id, community in enumerate(communities) for node in community}\n\n# Predict edges in the test set based on community membership\npredicted_edges_community = [(user, venue) for user in test_nodes for venue in train_nodes\n                              if community_assignment[user] == community_assignment[venue]]\n\n# Calculate precision and recall for Community Detection\ntrue_positive_community = len(set(predicted_edges_community).intersection(B.edges()))\nfalse_positive_community = len(set(predicted_edges_community) - set(B.edges()))\nfalse_negative_community = len(set(B.edges()) - set(predicted_edges_community))\n\nprecision_community = true_positive_community / (true_positive_community + false_positive_community) if (true_positive_community + false_positive_community) != 0 else 0\nrecall_community = true_positive_community / (true_positive_community + false_negative_community) if (true_positive_community + false_negative_community) != 0 else 0\n\nprint(f'Community Detection Score:')\nprint(f'Precision: {precision_community:.5f}')\nprint(f'Recall: {recall_community:.5f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.328647Z","iopub.status.idle":"2023-11-19T07:37:26.329266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n# Assuming you have loaded the datasets\n# checkins_df = pd.read_csv('NY_Restaurants_checkins.csv', header=None, names=['user_ID', 'venue_ID'])\n# tips_df = pd.read_csv('NY_Restaurants_tips.csv', header=None, names=['user_ID', 'venue_ID', 'tips'])\n# tags_df = pd.read_csv('NY_Restaurants_tags.csv', header=None, names=['venue_ID', 'tag_set'])\n\n# Assuming you have created a bipartite graph B\n\n# Generate train and test sets\ntrain_nodes, test_nodes = train_test_split(list(B.nodes()), test_size=0.2, random_state=42)\n\n# Create sets of venues visited by each user\nuser_venues = {user: set(B.neighbors(user)) for user in train_nodes}\n\n# Calculate Jaccard similarity between users based on common venues\nuser_jaccard_similarity = [\n    (u, v, len(user_venues[u].intersection(user_venues[v])) / len(user_venues[u].union(user_venues[v])))\n    if u in user_venues and v in user_venues\n    else (u, v, 0.0)  # Set similarity to 0 if either user is not present in user_venues\n    for u in test_nodes for v in train_nodes\n]\n\n# Calculate Preferential Attachment for each user-venue pair\npreferential_attachment = list(nx.preferential_attachment(B, [(u, v) for u in train_nodes for v in test_nodes]))\n\n# Predict edges in the test set based on Jaccard similarity and Preferential Attachment\nthreshold_jaccard = 0\npredicted_edges_jaccard = [(u, v) for u, v, p in user_jaccard_similarity if p > threshold_jaccard]\npredicted_edges_pa = [(u, v) for u, v, pa in preferential_attachment if pa > 10]\n\n# Calculate precision and recall for Jaccard and Preferential Attachment-based prediction\ntrue_positive_jaccard = len(set(predicted_edges_jaccard).intersection(B.edges()))\nfalse_positive_jaccard = len(set(predicted_edges_jaccard) - set(B.edges()))\nfalse_negative_jaccard = len(set(B.edges()) - set(predicted_edges_jaccard))\n\nprecision_jaccard = true_positive_jaccard / (true_positive_jaccard + false_positive_jaccard) if (true_positive_jaccard + false_positive_jaccard) != 0 else 0\nrecall_jaccard = true_positive_jaccard / (true_positive_jaccard + false_negative_jaccard) if (true_positive_jaccard + false_negative_jaccard) != 0 else 0\n\ntrue_positive_pa = len(set(predicted_edges_pa).intersection(B.edges()))\nfalse_positive_pa = len(set(predicted_edges_pa) - set(B.edges()))\nfalse_negative_pa = len(set(B.edges()) - set(predicted_edges_pa))\n\nprecision_pa = true_positive_pa / (true_positive_pa + false_positive_pa) if (true_positive_pa + false_positive_pa) != 0 else 0\nrecall_pa = true_positive_pa / (true_positive_pa + false_negative_pa) if (true_positive_pa + false_negative_pa) != 0 else 0\n\nprint(f'Jaccard-based Prediction:')\nprint(f'Precision: {precision_jaccard:}')\nprint(f'Recall: {recall_jaccard:}')\n\nprint(f'Preferential Attachment-based Prediction:')\nprint(f'Precision: {precision_pa:}')\nprint(f'Recall: {recall_pa:}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.330730Z","iopub.status.idle":"2023-11-19T07:37:26.331317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Create a graph\nG = nx.Graph()\n\n# Add edges with weights based on tips\nfor index, row in tips_df.iterrows():\n    user = row['user_ID']\n    venue = row['venue_ID']\n    tip = row['tips']\n\n    if G.has_edge(user, venue):\n        G[user][venue]['weight'] += 1\n    else:\n        G.add_edge(user, venue, weight=1, tips=[tip])\n\n# Create a matrix of user-venue interactions\nadjacency_matrix = nx.to_numpy_matrix(G, nodelist=G.nodes())\n\n# Calculate cosine similarity based on tips\ncosine_similarities = cosine_similarity(adjacency_matrix, adjacency_matrix)\n\n# Convert the cosine similarities to a graph\ncosine_graph = nx.from_numpy_array(cosine_similarities)\n\n# Calculate Jaccard similarity between users based on common venues\nuser_jaccard_similarity = [(u, v, len(set(G[u]) & set(G[v])) / len(set(G[u]) | set(G[v])))\n                           for u in G.nodes() for v in G.nodes()]\n\n# Combine the two similarity scores (cosine and Jaccard)\n# Combine the two similarity scores (cosine and Jaccard)\ncombined_similarity = []\n\nfor u, v, jaccard in user_jaccard_similarity:\n    if cosine_graph.has_edge(u, v):\n        weight_cosine = cosine_graph[u][v]['weight']\n        combined_similarity.append((u, v, 0.5 * weight_cosine + 0.5 * jaccard))\n\n# Set a threshold for the combined similarity\nthreshold = 0.2\npredicted_edges_combined = [(u, v) for u, v, sim in combined_similarity if sim > threshold]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.332657Z","iopub.status.idle":"2023-11-19T07:37:26.333407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a test set with actual positive edges named 'test_edges'\nactual_edges = set(test_edges)\npredicted_edges = set(predicted_edges_combined)\n\n# Calculate True Positives, False Positives, and False Negatives\ntrue_positives = len(actual_edges.intersection(predicted_edges))\nfalse_positives = len(predicted_edges - actual_edges)\nfalse_negatives = len(actual_edges - predicted_edges)\n\n# Calculate Precision and Recall\nprecision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\nrecall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.334638Z","iopub.status.idle":"2023-11-19T07:37:26.335353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Jaccard similarity between users based on common venues\nuser_jaccard_similarity = [\n    (u, v, len(user_venues[u].intersection(user_venues[v])) / len(user_venues[u].union(user_venues[v])))\n    if u in user_venues and v in user_venues and len(user_venues[u]) > 0 and len(user_venues[v]) > 0\n    else (u, v, 0.0)  # Set similarity to 0 if either user has not visited any venues or if the user ID is not in user_venues\n    for u in test_nodes for v in train_nodes\n]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.337047Z","iopub.status.idle":"2023-11-19T07:37:26.337795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,j,k in user_jaccard_similarity:\n    if k>0:\n        print(k)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.339378Z","iopub.status.idle":"2023-11-19T07:37:26.340094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sets of venues visited by each user\nuser_venues = {user: set(B.neighbors(user)) for user in train_nodes}\n\n# Print information about user sets for investigation\nprint(\"User Venues:\")\nfor user, venues in user_venues.items():\n    print(f\"{user}: {venues}\")\n\n# Calculate Jaccard similarity between users based on common venues\nuser_jaccard_similarity = [\n    (u, v, len(user_venues[u].intersection(user_venues[v])) / len(user_venues[u].union(user_venues[v])))\n    if u in user_venues and v in user_venues\n    else (u, v, 0.0)  # Set similarity to 0 if either user is not present in user_venues\n    for u in test_nodes for v in train_nodes\n]\n\n# Print Jaccard similarities for investigation\nprint(\"\\nJaccard Similarities:\")\nfor u, v, p in user_jaccard_similarity:\n    if p>0:\n        print(f\"Jaccard({u}, {v}) = {p}\")\n\n# ... rest of the code remains unchanged\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.341511Z","iopub.status.idle":"2023-11-19T07:37:26.342182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor user in test_nodes:\n    for venue in train_nodes:\n        if(count==50):\n            break\n        count=count+1\n        print(negative_shortest_distance_score(B, user, venue))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.343533Z","iopub.status.idle":"2023-11-19T07:37:26.344211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor user in test_nodes:\n    for venue in train_nodes:\n        if(calculate_combined_score(user, venue)>1):\n            print(calculate_combined_score(user, venue))\n#         common_neighbors_venue_score(B, user, venue)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:37:26.345852Z","iopub.status.idle":"2023-11-19T07:37:26.346413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}